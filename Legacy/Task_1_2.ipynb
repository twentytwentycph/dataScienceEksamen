{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'id', 'domain', 'type', 'url', 'content', 'scraped_at',\n",
       "       'inserted_at', 'updated_at', 'title', 'authors', 'keywords',\n",
       "       'meta_keywords', 'meta_description', 'tags', 'summary'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the libraries that we will use in this task\n",
    "# You can install them using the following command:\n",
    "# pip install clean-text\n",
    "# pip install pandas\n",
    "import re\n",
    "import pandas as pd\n",
    "from cleantext import clean \n",
    "import cleantext\n",
    "\n",
    "data = 'https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv'\n",
    "df = pd.read_csv(data) # Read the data into a DataFrame\n",
    "\n",
    "# A subset of the DataFrame containing the rows from 100 to 200\n",
    "subset_df = df.loc[100:200] \n",
    "\n",
    "# Regular expressions to match different types of URLs\n",
    "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+|\\S+\\.com')\n",
    "# Regular expressions to match the date format\n",
    "date_pattern = re.compile(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}.\\d{6})|'            # YYYY-MM-DD HH:MM:SS.MMMMMM\n",
    "                        r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})|'                      # YYYY-MM-DD HH:MM:SS\n",
    "                        r'(\\d{4}-\\d{2}-\\d{2})|'                                        # YYYY-MM-DD\n",
    "                        r'(\\d{4}\\.\\d{2}\\.\\d{2})|'                                      # YYYY.MM.DD \n",
    "                        r'(\\d{2}\\.\\d{2}\\.\\d{4})|'                                      # DD.MM.YYYY\n",
    "                        r'(\\d{4}/\\d{2}/\\d{2})|'                                        # YYYY/MM/DD\n",
    "                        r'(\\d{2}/\\d{2}/\\d{4})|'                                        # DD/MM/YYYY\n",
    "                        r'((january|february|march|april|june|july|august|september|'  # <Month> DD YYYY\n",
    "                        r'october|november|december) \\d{2}, \\d{4})', re.IGNORECASE)    # Ignore capitalization\n",
    "# Regular expression to match numbers.\n",
    "number_pattern = re.compile(r'(\\d+(?:,\\d{3})*(?:\\.\\d+)?)')\n",
    "\n",
    "\n",
    "def cleaner_text(df):\n",
    "    \n",
    "    # Identify string columns\n",
    "    string_columns = df.select_dtypes(include=[\"object\"]).columns # his line finds all columns in the DataFrame df that have an object data type usually strings\n",
    "\n",
    "    for col in string_columns:\n",
    "        # Use vectorized string operations from pandas\n",
    "        df[col] = (\n",
    "            df[col]\n",
    "            .replace(r'\\s+', ' ', regex=True)\n",
    "            .replace(url_pattern, '<URL>', regex=True)\n",
    "            .replace(date_pattern, '<DATE>', regex=True)\n",
    "            .replace(number_pattern, '<NUM>', regex=True)\n",
    "            .apply(clean)\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "subset_df.columns\n",
    "#subset_df\n",
    "\n",
    "#subset_df.drop(['id',''])\n",
    "\n",
    "\n",
    "\n",
    "#subset_df.drop('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# A function to plot the top 50 most frequent words\\ndef plot(words, title, colors, max_words):\\n    # count the frequency of each word\\n    word_counts = words.value_counts()\\n\\n    # sort the list of words by frequency\\n    word_counts = word_counts.sort_values(ascending=False)\\n\\n    # plot a bar chart of the 50 most frequent words\\n    top_words = word_counts[:max_words] # get the top 50 words\\n    fig, ax = plt.subplots(figsize=(12, 6))\\n    ax.bar(top_words.index, top_words.values, color=colors, width=0.8, align='edge') \\n    ax.set_xticks(range(len(top_words))) # set the x-ticks to the word positions\\n    ax.set_xticklabels(top_words.index, rotation=90)\\n    ax.set_xlabel('Word/Tokens')\\n    ax.set_ylabel('Frequency of Occurrence')\\n    ax.set_xlim(-0.5, len(top_words) - 0.5) # add a gap at the beginning and end\\n    plt.title(title)\\n    plt.show()\\n\\n\\n\\n### DF info ###\\nprint('\\n### Unique words in the data before and after preprocessing ###\\n')\\nprint('Unique words before preprocessing: ', word_info(preDf, 'ints')) # 31808\\nprint('Unique words after preprocessing: ', word_info(postDf, 'ints')) # 26628\\nprint('Unique words after removing stopwords: ', word_info(nostopword_df, 'ints')) \\nprint('')\\nprint('Reduction in size of vocabulary: ', (word_info(postDf, 'ints')) - (word_info(nostopword_df, 'ints')))\\nprint('Reduction %: ', round(((word_info(postDf, 'ints')) - (word_info(nostopword_df, 'ints')))/word_info(postDf, 'ints')*100, 5), '%')\\nprint('')\\nprint('Reduction in size of vocabulary after stemming: ', (word_info(postDf, 'ints')) - (word_info(stemmed_df, 'ints')))\\nprint('Reduction %: ', round(((word_info(postDf, 'ints')) - (word_info(stemmed_df, 'ints')))/word_info(postDf, 'ints')*100, 5), '%')\\nprint('')\\nprint('Unique words after stemming & removing stopwords: ', word_info(stemmed_df, 'ints'))\\n\\n### Plots ###\\nplot(word_info(preDf, 'words'), 'Top 50 most frequent words in the data before preprocessing', 'red', 50)\\nplot(word_info(postDf, 'words'), 'Top 50 most frequent words in the data after preprocessing', 'green', 50)\\nplot(word_info(nostopword_df, 'words'), 'Top 50 most frequent words in the data after removing stopwords', 'blue', 50)\\n\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import PorterStemmer\n",
    "import timeit\n",
    "ps = PorterStemmer()\n",
    "\n",
    "preData = 'https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv'\n",
    "preDf = pd.read_csv(data) # Read the unprocessed data into a DataFrame\n",
    "postDf = cleaner_text(df) # Read the processed data into a DataFrame\n",
    "\n",
    "# A function to get the number of words and the words themselves in each column of a DataFrame\n",
    "# The function takes two arguments: the DataFrame and an option\n",
    "# The option can be either 'ints' or 'words' if we want the number of words or the words themselves\n",
    "def word_info(df, opt):\n",
    "    text_series = pd.concat([df[col] for col in df.columns], ignore_index=True) # concatenate all the text columns into a single series\n",
    "    if opt == 'ints':\n",
    "        words_int = text_series.str.split().explode().nunique() # count the number of unique words\n",
    "        return words_int\n",
    "    elif opt == 'words':\n",
    "        words = text_series.str.split().explode() # split the text into a series of words\n",
    "        return words\n",
    "\n",
    "# Count the frequency of each word and \n",
    "def wordSignificance(word):\n",
    "    word_counts = word.value_counts() # Count the frequency of each word\n",
    "    # Calculate the significance of each word\n",
    "    # By dividing the frequency of each word by the total number of words\n",
    "    word_counts = word_counts / len(word)\n",
    "    return word_counts\n",
    "\n",
    "# A function to get the top n% stopwords \n",
    "def topStopwords(wordSignificance, n):\n",
    "    # Get the words that occur more than n% of the time\n",
    "    stopword = wordSignificance[wordSignificance > n]\n",
    "    stopword_list = stopword.index.tolist() # Convert the index to a list\n",
    "    return stopword_list # Return the list of stopwords\n",
    "topstopword_list = topStopwords(wordSignificance(word_info(postDf, 'words')),0.005)\n",
    "\n",
    "# A function to get the bottom n% stopwords\n",
    "def botStopwords(wordSignificance, n):\n",
    "    # Get the words that occur more than n% of the time\n",
    "    stopword = wordSignificance[wordSignificance < n]\n",
    "    stopword_list = stopword.index.tolist() # Convert the index to a list\n",
    "    return stopword_list # Return the list of stopwords\n",
    "botstopword_list = botStopwords(wordSignificance(word_info(postDf, 'words')),0.00001)\n",
    "\n",
    "\n",
    "# A function to remove stopwords from our dataframe\n",
    "def removeStopwords(stopwords, df_WIP):\n",
    "    #df_WIP = df.copy() # Make a copy of the DataFrame for use in this function\n",
    "    for col in df_WIP.columns: # Loop over all columns in the DataFrame\n",
    "        # If any of the stopwords are in the text, replace them with an empty string\n",
    "        if df_WIP[col].dtype == 'O': # Check if the column is a string (i.e., object or string dtype)\n",
    "            # remove stopwords from the text in each column of the DataFrame using the apply function and lambda expression\n",
    "            df_WIP[col] = df_WIP[col].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords]))\n",
    "    return df_WIP\n",
    "nostopword_df = removeStopwords(topstopword_list + botstopword_list, postDf) # Read the data without stopwords into a DataFrame\n",
    "\n",
    "\n",
    "def stem_word(word):\n",
    "    return ' '.join([ps.stem(w) for w in word.split()])\n",
    "\n",
    "def df_stemmer(df_WIP):\n",
    "    # Only select 'object' dtype columns\n",
    "    object_columns = df_WIP.select_dtypes(include=['object'])\n",
    "\n",
    "    # Apply stemming to every element in the object_columns DataFrame\n",
    "    stemmed_columns = object_columns.applymap(stem_word)\n",
    "\n",
    "    # Update the original DataFrame with the stemmed columns\n",
    "    df_WIP.update(stemmed_columns)\n",
    "\n",
    "    return df_WIP\n",
    "\n",
    "stemmed_df = df_stemmer(nostopword_df)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# A function to plot the top 50 most frequent words\n",
    "def plot(words, title, colors, max_words):\n",
    "    # count the frequency of each word\n",
    "    word_counts = words.value_counts()\n",
    "\n",
    "    # sort the list of words by frequency\n",
    "    word_counts = word_counts.sort_values(ascending=False)\n",
    "\n",
    "    # plot a bar chart of the 50 most frequent words\n",
    "    top_words = word_counts[:max_words] # get the top 50 words\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.bar(top_words.index, top_words.values, color=colors, width=0.8, align='edge') \n",
    "    ax.set_xticks(range(len(top_words))) # set the x-ticks to the word positions\n",
    "    ax.set_xticklabels(top_words.index, rotation=90)\n",
    "    ax.set_xlabel('Word/Tokens')\n",
    "    ax.set_ylabel('Frequency of Occurrence')\n",
    "    ax.set_xlim(-0.5, len(top_words) - 0.5) # add a gap at the beginning and end\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "### DF info ###\n",
    "print('\\n### Unique words in the data before and after preprocessing ###\\n')\n",
    "print('Unique words before preprocessing: ', word_info(preDf, 'ints')) # 31808\n",
    "print('Unique words after preprocessing: ', word_info(postDf, 'ints')) # 26628\n",
    "print('Unique words after removing stopwords: ', word_info(nostopword_df, 'ints')) \n",
    "print('')\n",
    "print('Reduction in size of vocabulary: ', (word_info(postDf, 'ints')) - (word_info(nostopword_df, 'ints')))\n",
    "print('Reduction %: ', round(((word_info(postDf, 'ints')) - (word_info(nostopword_df, 'ints')))/word_info(postDf, 'ints')*100, 5), '%')\n",
    "print('')\n",
    "print('Reduction in size of vocabulary after stemming: ', (word_info(postDf, 'ints')) - (word_info(stemmed_df, 'ints')))\n",
    "print('Reduction %: ', round(((word_info(postDf, 'ints')) - (word_info(stemmed_df, 'ints')))/word_info(postDf, 'ints')*100, 5), '%')\n",
    "print('')\n",
    "print('Unique words after stemming & removing stopwords: ', word_info(stemmed_df, 'ints'))\n",
    "\n",
    "### Plots ###\n",
    "plot(word_info(preDf, 'words'), 'Top 50 most frequent words in the data before preprocessing', 'red', 50)\n",
    "plot(word_info(postDf, 'words'), 'Top 50 most frequent words in the data after preprocessing', 'green', 50)\n",
    "plot(word_info(nostopword_df, 'words'), 'Top 50 most frequent words in the data after removing stopwords', 'blue', 50)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(stemmer(nostopword_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemmed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m cleaned_chunk \u001b[39m=\u001b[39m cleaner_text(chunk\u001b[39m.\u001b[39mcopy())\n\u001b[1;32m     15\u001b[0m no_stopwords_chunk \u001b[39m=\u001b[39m removeStopwords(topstopword_list \u001b[39m+\u001b[39m botstopword_list, cleaned_chunk) \n\u001b[0;32m---> 16\u001b[0m stemmed_chunk \u001b[39m=\u001b[39m df_stemmer(no_stopwords_chunk) \n\u001b[1;32m     18\u001b[0m \u001b[39m# Write the cleaned chunk to the output file\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39mif\u001b[39;00m first_chunk:\n",
      "Cell \u001b[0;32mIn[9], line 68\u001b[0m, in \u001b[0;36mdf_stemmer\u001b[0;34m(df_WIP)\u001b[0m\n\u001b[1;32m     65\u001b[0m object_columns \u001b[39m=\u001b[39m df_WIP\u001b[39m.\u001b[39mselect_dtypes(include\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     67\u001b[0m \u001b[39m# Apply stemming to every element in the object_columns DataFrame\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m stemmed_columns \u001b[39m=\u001b[39m object_columns\u001b[39m.\u001b[39;49mapplymap(stem_word)\n\u001b[1;32m     70\u001b[0m \u001b[39m# Update the original DataFrame with the stemmed columns\u001b[39;00m\n\u001b[1;32m     71\u001b[0m df_WIP\u001b[39m.\u001b[39mupdate(stemmed_columns)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/core/frame.py:9650\u001b[0m, in \u001b[0;36mDataFrame.applymap\u001b[0;34m(self, func, na_action, **kwargs)\u001b[0m\n\u001b[1;32m   9647\u001b[0m         \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39mmap_infer(x, func, ignore_na\u001b[39m=\u001b[39mignore_na)\n\u001b[1;32m   9648\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39mmap_infer(x\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values, func, ignore_na\u001b[39m=\u001b[39mignore_na)\n\u001b[0;32m-> 9650\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(infer)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mapplymap\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/core/frame.py:9565\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9554\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9556\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[1;32m   9557\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   9558\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9563\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[1;32m   9564\u001b[0m )\n\u001b[0;32m-> 9565\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/core/apply.py:746\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[1;32m    744\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[0;32m--> 746\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/core/apply.py:873\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 873\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[1;32m    875\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[1;32m    876\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/core/apply.py:889\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    887\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[1;32m    888\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[1;32m    890\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    891\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    892\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    893\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/core/frame.py:9648\u001b[0m, in \u001b[0;36mDataFrame.applymap.<locals>.infer\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   9646\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mempty:\n\u001b[1;32m   9647\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39mmap_infer(x, func, ignore_na\u001b[39m=\u001b[39mignore_na)\n\u001b[0;32m-> 9648\u001b[0m \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39;49mmap_infer(x\u001b[39m.\u001b[39;49mastype(\u001b[39mobject\u001b[39;49m)\u001b[39m.\u001b[39;49m_values, func, ignore_na\u001b[39m=\u001b[39;49mignore_na)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2918\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[9], line 61\u001b[0m, in \u001b[0;36mstem_word\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstem_word\u001b[39m(word):\n\u001b[0;32m---> 61\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([ps\u001b[39m.\u001b[39mstem(w) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m word\u001b[39m.\u001b[39msplit()])\n",
      "Cell \u001b[0;32mIn[9], line 61\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstem_word\u001b[39m(word):\n\u001b[0;32m---> 61\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([ps\u001b[39m.\u001b[39;49mstem(w) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m word\u001b[39m.\u001b[39msplit()])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/nltk/stem/porter.py:675\u001b[0m, in \u001b[0;36mPorterStemmer.stem\u001b[0;34m(self, word, to_lowercase)\u001b[0m\n\u001b[1;32m    673\u001b[0m stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step3(stem)\n\u001b[1;32m    674\u001b[0m stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step4(stem)\n\u001b[0;32m--> 675\u001b[0m stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step5a(stem)\n\u001b[1;32m    676\u001b[0m stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step5b(stem)\n\u001b[1;32m    678\u001b[0m \u001b[39mreturn\u001b[39;00m stem\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_file = '/volumes/Glyph1TB/newsCorpus/news_cleaned_2018_02_13.csv'\n",
    "output_file = '/volumes/Glyph1TB/newsCorpus/news_cleaned_2018_02_13-results.csv'\n",
    "\n",
    "#input_file = '/volumes/Glyph1TB/newsCorpus/Wiki_news.csv'\n",
    "#output_file = '/volumes/Glyph1TB/newsCorpus/Wiki_news_cleaned.csv'\n",
    "\n",
    "chunksize = 1000 \n",
    "\n",
    "\n",
    "with pd.read_csv(input_file, chunksize=chunksize, encoding='utf-8', lineterminator='\\n') as reader:\n",
    "    with open(output_file, \"a\") as out:\n",
    "        first_chunk = True\n",
    "        for chunk in reader:\n",
    "            cleaned_chunk = cleaner_text(chunk.copy())\n",
    "            no_stopwords_chunk = removeStopwords(topstopword_list + botstopword_list, cleaned_chunk) \n",
    "            stemmed_chunk = df_stemmer(no_stopwords_chunk) \n",
    "            \n",
    "            # Write the cleaned chunk to the output file\n",
    "            if first_chunk:\n",
    "                stemmed_chunk.to_csv(out, index=False, encoding='utf-8')\n",
    "                first_chunk = False\n",
    "            else:\n",
    "                stemmed_chunk.to_csv(out, mode='a', header=False, index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "008e33b4549f1a0d0ffe974fe84e17b627f351b479525f2395e64f0548f2c8e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
