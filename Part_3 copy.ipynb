{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = sparse.load_npz('csv/sparse_matrix_for_text.npz')\n",
    "title = sparse.load_npz('csv/sparse_matrix_for_title.npz') \n",
    "\n",
    "collected_matrix = hstack([content,title]) \n",
    "sparse.save_npz(\"csv/sparse_matrix_for_all.npz\", collected_matrix)\n",
    "\n",
    "labels = pd.read_csv('csv/labels.csv')\n",
    "features = sparse.load_npz('csv/sparse_matrix_for_all.npz')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- changing to linearSVC for faster training time instead of SVM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  7,  7, ..., 10, 10, 10])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = features\n",
    "y = labels \n",
    "y = np.ravel(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9959242010404491\n",
      "0.8434984031772236\n",
      "Naive Bayes Accuracy Score ->  84.34984031772235\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.90     49640\n",
      "           1       0.46      0.84      0.59      2638\n",
      "           2       0.90      0.95      0.92     63216\n",
      "           3       0.77      0.72      0.75      5781\n",
      "           4       0.85      0.68      0.76     59590\n",
      "           5       0.90      0.79      0.84     57876\n",
      "           6       0.55      0.90      0.68      9241\n",
      "           7       0.49      0.90      0.63     17394\n",
      "           8       0.56      0.70      0.62      8626\n",
      "           9       1.00      0.98      0.99     68565\n",
      "          10       0.87      0.78      0.82    120227\n",
      "\n",
      "    accuracy                           0.84    462794\n",
      "   macro avg       0.75      0.83      0.77    462794\n",
      "weighted avg       0.86      0.84      0.85    462794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report \n",
    "from imblearn.over_sampling import RandomOverSampler \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.5, random_state=0)\n",
    "\n",
    "oversampler = RandomOverSampler(random_state=0)\n",
    "X_train_oversampled, y_train_oversampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "model = ComplementNB() \n",
    "model.fit(X_train_oversampled, y_train_oversampled)\n",
    "\n",
    "print(model.score(X_train_oversampled, y_train_oversampled))\n",
    "print(model.score(X_val, y_val))\n",
    "\n",
    "predictions_NB = model.predict(X_val)\n",
    "\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, y_val)*100)\n",
    "\n",
    "y_val_pred = model.predict(X_val) \n",
    "\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "print(\"classification report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nfrom sklearn.linear_model import LogisticRegression\\n\\nfrom sklearn.metrics import accuracy_score\\n\\n# Define a range of C values to test\\nC_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\\n\\n# Initialize the best_C and best_score variables\\nbest_C = None\\nbest_score = 0\\n\\n# Iterate through the C_values and fit the model with each value\\nfor C in C_values:\\n    # Create a Logistic Regression model with the current C value\\n    model = LogisticRegression(max_iter=1500, class_weight = \\'balanced\\', random_state = 0, C=C)\\n    \\n    # Fit the model to the training data\\n    model.fit(xv_train, y_train)\\n    \\n    # Predict the validation data\\n    y_val_pred = model.predict(xv_val)\\n    \\n    # Calculate the accuracy score for the current C value\\n    score = accuracy_score(y_val, y_val_pred)\\n    \\n    # Print the current C value and its accuracy score\\n    print(f\"C: {C}, Accuracy: {score}\")\\n    \\n    # Update the best_C and best_score variables if the current score is higher than the previous best\\n    if score > best_score:\\n        best_C = C\\n        best_score = score\\n\\n# Print the best C value and its accuracy score\\nprint(f\"Best C: {best_C}, Best Accuracy: {best_score}\")\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define a range of C values to test\n",
    "C_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "# Initialize the best_C and best_score variables\n",
    "best_C = None\n",
    "best_score = 0\n",
    "\n",
    "# Iterate through the C_values and fit the model with each value\n",
    "for C in C_values:\n",
    "    # Create a Logistic Regression model with the current C value\n",
    "    model = LogisticRegression(max_iter=1500, class_weight = 'balanced', random_state = 0, C=C)\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    model.fit(xv_train, y_train)\n",
    "    \n",
    "    # Predict the validation data\n",
    "    y_val_pred = model.predict(xv_val)\n",
    "    \n",
    "    # Calculate the accuracy score for the current C value\n",
    "    score = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Print the current C value and its accuracy score\n",
    "    print(f\"C: {C}, Accuracy: {score}\")\n",
    "    \n",
    "    # Update the best_C and best_score variables if the current score is higher than the previous best\n",
    "    if score > best_score:\n",
    "        best_C = C\n",
    "        best_score = score\n",
    "\n",
    "# Print the best C value and its accuracy score\n",
    "print(f\"Best C: {best_C}, Best Accuracy: {best_score}\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 1e-05, Accuracy: 0.8335285245703272\n",
      "alpha: 0.0001, Accuracy: 0.8371910612497137\n",
      "alpha: 0.001, Accuracy: 0.8420463532370774\n",
      "alpha: 0.01, Accuracy: 0.8477141017385705\n",
      "alpha: 0.1, Accuracy: 0.8516813096107555\n",
      "alpha: 1, Accuracy: 0.8434984031772236\n",
      "alpha: 10, Accuracy: 0.8146497145598257\n",
      "alpha: 100, Accuracy: 0.7805546312182094\n",
      "alpha: 1000, Accuracy: 0.7472266278300929\n",
      "alpha: 10000, Accuracy: 0.6992268698384162\n",
      "\n",
      "Best alpha: 0.1, Best Accuracy: 0.8516813096107555\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "# Define the parameter grid for the grid search\n",
    "param_grid = [0.00001,0.0001,0.001, 0.01,0.1,1,10,100,1000,10000]\n",
    "\n",
    "# Initialize the best_C and best_score variables\n",
    "best_alpha = None\n",
    "best_score = 0\n",
    "\n",
    "for pg in param_grid:\n",
    "    model = ComplementNB(alpha=pg)\n",
    "    \n",
    "    model.fit(X_train_oversampled, y_train_oversampled)\n",
    "    \n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate the accuracy score for the current C value\n",
    "    score = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Print the current C value and its accuracy score\n",
    "    print(f\"alpha: {pg}, Accuracy: {score}\")\n",
    "    \n",
    "    # Update the best_C and best_score variables if the current score is higher than the previous best\n",
    "    if score > best_score:\n",
    "        best_alpha = pg\n",
    "        best_score = score\n",
    "    \n",
    "# Print the best C value and its accuracy score\n",
    "print(f\"\\nBest alpha: {best_alpha}, Best Accuracy: {best_score}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found: {'alpha': 0.001}\n",
      "Validation accuracy with tuned hyperparameters: 0.81884596602376\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Define the parameter grid for the grid search\n",
    "param_grid = {'alpha': [0.00001,0.0001,0.001, 0.01,0.1,1,10,100,1000,10000]}\n",
    "\n",
    "# Create the MultinomialNB model\n",
    "model = MultinomialNB()\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search object to the oversampled training data\n",
    "grid_search.fit(X_train_oversampled, y_train_oversampled)\n",
    "\n",
    "# Get the best hyperparameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best hyperparameters found:\", best_params)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = MultinomialNB(**best_params)\n",
    "best_model.fit(X_train_oversampled, y_train_oversampled)\n",
    "\n",
    "# Evaluate the model\n",
    "val_accuracy = best_model.score(X_val, y_val)\n",
    "print(\"Validation accuracy with tuned hyperparameters:\", val_accuracy)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9995912088710703\n",
      "0.7245102572634908\n",
      "Naive Bayes Accuracy Score ->  72.45102572634909\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.44      0.55     49640\n",
      "           1       0.93      0.28      0.43      2638\n",
      "           2       0.90      0.86      0.88     63216\n",
      "           3       0.96      0.40      0.56      5781\n",
      "           4       0.80      0.52      0.63     59590\n",
      "           5       0.82      0.61      0.70     57876\n",
      "           6       0.89      0.54      0.67      9241\n",
      "           7       0.80      0.17      0.28     17394\n",
      "           8       0.89      0.26      0.40      8626\n",
      "           9       0.92      0.97      0.95     68565\n",
      "          10       0.55      0.93      0.69    120227\n",
      "\n",
      "    accuracy                           0.72    462794\n",
      "   macro avg       0.84      0.54      0.61    462794\n",
      "weighted avg       0.77      0.72      0.71    462794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report \n",
    "from imblearn.over_sampling import RandomOverSampler \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.5, random_state=0)\n",
    "\n",
    "oversampler = RandomOverSampler(random_state=0)\n",
    "X_train_oversampled, y_train_oversampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "model = MultinomialNB(**best_params) \n",
    "model.fit(X_train_oversampled, y_train_oversampled)\n",
    "\n",
    "\n",
    "\n",
    "print(model.score(X_train_oversampled, y_train_oversampled))\n",
    "print(model.score(X_val, y_val))\n",
    "\n",
    "predictions_NB = model.predict(X_val)\n",
    "\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, y_val)*100)\n",
    "\n",
    "y_val_pred = model.predict(X_val) \n",
    "\n",
    "report = classification_report(y_val, y_val_pred)\n",
    "print(\"classification report:\")\n",
    "print(report)\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a6f82daab6db77363711a6fc909ceb02ec1e572b8a891fc851695512128acb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
