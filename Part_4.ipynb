{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now evaluate your models on the FakeNews and the LIAR dataset. Arrange all these results in a table to facilitate a comparison between them. You should be evaluating the model on how well it classifies articles correctly using F-score. You may want to include a confusion matrix to visualize the types of classification errors made by your models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "Evaluate the performance of your Simple and Advanced Models on your FakeNewsCorpus test set. It should be possible to achieve > 80% accuracy but you will not fail the project if your model cannot reach this performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "# pip install dask-ml\n",
    "import dask.dataframe as dd\n",
    "import dask_ml.model_selection\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a Dask DataFrame with specified data types\n",
    "# These dtypes can be found by printing the dask dataframe\n",
    "cleaned_data = dd.read_csv('data_3GB.csv', encoding=\"utf-8\", dtype={\n",
    "        'Unnamed: 0': 'object',\n",
    "        'id': 'object',\n",
    "        'domain': 'object',\n",
    "        'type': 'object',\n",
    "        'url': 'object',\n",
    "        'content': 'object',\n",
    "        'scraped_at': 'object',\n",
    "        'inserted_at': 'object',\n",
    "        'updated_at': 'object',\n",
    "        'title': 'object',\n",
    "        'authors': 'object',\n",
    "        'keywords': 'float64',\n",
    "        'meta_keywords': 'object',\n",
    "        'meta_description': 'object',\n",
    "        'tags': 'object',\n",
    "        'summary': 'float64',\n",
    "        'tokens': 'object',\n",
    "        'filtered_tokens': 'object',\n",
    "        'stemmed_tokens': 'object',\n",
    "    },)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to modify the 'type' column values\n",
    "# This function is used to simplify the classification problem\n",
    "# The 'reliable' and 'political' types are combined into a single 'reliable' type\n",
    "def modify_type(x):\n",
    "    if x == 'reliabl' or x == 'polit':\n",
    "        return 'reliable'\n",
    "    else:\n",
    "        return 'fake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the modify_type function to the 'type' column using the map function\n",
    "cleaned_data['type'] = cleaned_data['type'].map(modify_type, meta=('type', 'object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify if there are any missing values in the 'type' column\n",
    "# If so, they are replaced with an empty string\n",
    "def nan_to_empty(x):\n",
    "    if isinstance(x, float) and np.isnan(x):\n",
    "        return ''\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the nan_to_empty function to the 'type' column using the map function\n",
    "cleaned_data['content'] = cleaned_data['content'].map(nan_to_empty, meta=('content', 'object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y as the 'content' and 'type' columns\n",
    "y = cleaned_data['type']\n",
    "X = cleaned_data['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "# In an 80/10/10 split\n",
    "\n",
    "X_train, X_test, y_train, y_test = dask_ml.model_selection.train_test_split(X, y, test_size=0.2, random_state=0, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = dask_ml.model_selection.train_test_split(X_train, y_train, test_size=0.125, random_state=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TfidfVectorizer object for use in feature extraction\n",
    "vectorization = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the training data and transform the training data into a vector\n",
    "# As well as the validation data\n",
    "xv_train = vectorization.fit_transform(X_train)\n",
    "xv_test = vectorization.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=1500, class_weight = 'balanced', random_state = 0, C=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Agnes/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100, class_weight='balanced', max_iter=1500,\n",
       "                   random_state=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xv_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8993657658967416"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the accuracy of the model on the validation data\n",
    "model.score(xv_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class of the validation data\n",
    "pred_model = model.predict(xv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.96      0.91      0.93    213848\n",
      "    reliable       0.73      0.88      0.80     61287\n",
      "\n",
      "    accuracy                           0.90    275135\n",
      "   macro avg       0.84      0.89      0.86    275135\n",
      "weighted avg       0.91      0.90      0.90    275135\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print the classification report for the model\n",
    "print(classification_report(y_test,pred_model))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "In order to allow you to play around cross-domain performance, try the same exercise on the LIAR dataset, where you know the labels, and can thus immediately calculate the performance. You are expected to directly evaluate the model you trained on the FakeNewsCorpus. In other words, you do not need to retrain the model on the LIAR dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "Compare the results of this experiment to the results you obtained in question 3. Report your LIAR results as part of your report. Remember to test the performance of your Simple Model as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
