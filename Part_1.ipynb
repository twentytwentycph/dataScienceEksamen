{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake News Project\n",
    "\n",
    "By: Mateo Anusic, Emil Thorlund, Lucas A. Rosing, Victor Bergien"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #1\n",
    "\n",
    "- Tokenize the text\n",
    "- Remove stopwords and compute the size of the vocabulary.\n",
    "- Compute the reduction rate of the vocabulary size after removing stopwords.\n",
    "- Remove word variations with stemming and compute the size of the vocabulary.\n",
    "- Compute the reduction rate of the vocabulary size after stemming.\n",
    "\n",
    "Describe which procedures (and which libraries) you used and why they are appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code ###\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from cleantext import clean \n",
    "import cleantext\n",
    "import csv\n",
    "import requests\n",
    "from io import StringIO\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "data_url = 'https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv'\n",
    "\n",
    "#nltk.download('punkt')\n",
    "\n",
    "response = requests.get(data_url)\n",
    "response.raise_for_status()  #Raise exeption\n",
    "\n",
    "csv_data = response.content.decode('utf-8')\n",
    "csv_file = StringIO(csv_data)\n",
    "\n",
    "reader = csv.DictReader(csv_file)\n",
    "\n",
    "start_row = 100\n",
    "end_row = 102\n",
    "\n",
    "subset_rows = list(islice(reader, start_row, end_row))\n",
    "\n",
    "#for row_number, row in enumerate(subset_rows, start=start_row):\n",
    "#    print(f\"Row {row_number}:\")\n",
    "#    for column_name, cell_value in row.items():\n",
    "#        print(f\"  {column_name}: {cell_value}\")\n",
    "#    print()  # Print an empty line to separate rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_pattern = re.compile(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}.\\d{6})|'            # YYYY-MM-DD HH:MM:SS.MMMMMM\n",
    "                        r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})|'                      # YYYY-MM-DD HH:MM:SS\n",
    "                        r'(\\d{4}-\\d{2}-\\d{2})|'                                        # YYYY-MM-DD\n",
    "                        r'(\\d{4}\\.\\d{2}\\.\\d{2})|'                                      # YYYY.MM.DD \n",
    "                        r'(\\d{2}\\.\\d{2}\\.\\d{4})|'                                      # DD.MM.YYYY\n",
    "                        r'(\\d{4}/\\d{2}/\\d{2})|'                                        # YYYY/MM/DD\n",
    "                        r'(\\d{2}/\\d{2}/\\d{4})|'                                        # DD/MM/YYYY\n",
    "                        r'((january|february|march|april|june|july|august|september|'  # <Month> DD YYYY\n",
    "                        r'october|november|december) \\d{2}, \\d{4})', re.IGNORECASE)  \n",
    "number_pattern = re.compile(r'(\\d+(?:,\\d{3})*(?:\\.\\d+)?)')\n",
    "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+|\\S+\\.com')\n",
    "\n",
    "def clean_text(read):\n",
    "    read = read.lower()\n",
    "    read = re.sub(r\"\\s+\", \" \", read)\n",
    "    read = re.sub(date_pattern, '<DATE>', read)\n",
    "    read = re.sub(number_pattern, \"<NUM>\", read)\n",
    "    read = re.sub(r\"\\S+@\\S+\", \"<EMAIL>\", read)\n",
    "    read = re.sub(url_pattern, \"<URL>\", read)\n",
    "    return read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 100:\n",
      "  : <NUM>\n",
      "  id: <NUM>\n",
      "  domain: <URL>\n",
      "  type: fake\n",
      "  url: http : <URL>/libertarian/<NUM>/<NUM>/greenmedinfo-action-item-link-<NUM>-<NUM>.html\n",
      "  content: greenmedinfo – action item link % of readers think this story is fact . add your two cents . headline : bitcoin & blockchain searches exceed trump ! blockchain stocks are next ! one of the links in the greedmedinfo update was incomplete . the letter writing campaign is located here : make the fda advisory , not mandatory source : https : //downsizedc.org/blog/greenmedinfo-action-item-link\n",
      "  scraped_at: <DATE> <NUM>:<NUM>:<NUM>\n",
      "  inserted_at: <DATE> <NUM>:<NUM>:<NUM>\n",
      "  updated_at: <DATE> <NUM>:<NUM>:<NUM>\n",
      "  title: greenmedinfo – action item link\n",
      "  authors: downsize dc\n",
      "  keywords: \n",
      "  meta_keywords: [ `` ]\n",
      "  meta_description: \n",
      "  tags: \n",
      "  summary: \n",
      "\n",
      "Row 101:\n",
      "  : <NUM>\n",
      "  id: <NUM>\n",
      "  domain: <URL>\n",
      "  type: fake\n",
      "  url: http : <URL>/business/<NUM>/<NUM>/<NUM>-most-annoying-twitter-auto-dms-<NUM>.html\n",
      "  content: <NUM> most annoying twitter auto dms headline : bitcoin & blockchain searches exceed trump ! blockchain stocks are next ! have you seen “ cheap supplements <NUM> you ” or “ register in my business program and you ’ ll receive endless benefits. ” you most likely thought this was email spam . they ’ re real examples of spam , but from twitter . last week i wrote about the <NUM> worst social media mistakes . one of those mistakes is annoying auto direct messages . you may receive one of these when you engage with someone ’ s account by following them or liking a tweet of theirs . immediately after you click “ follow ” , a program automatically sends you a pre-written message . i ’ m not entirely against the idea of auto dm services , but it ’ s gotten completely out of hand and is mostly used irresponsibly . the end result is an inbox saturated with pointless messages , forcing your authentic one-on-one interactions to the bottom . here are the worst seven messages i ’ ve received : hola . gracias por seguirme . me dejas saber si hay algo en que lo puedo ayudar a innovar ! if you ’ re going to send anyone a message , make sure it ’ s in the person ’ s native speaking language . even when translated , the message is the standard “ thanks , let me know if i can help. ” i ’ m not sure how many people are asking for help from strangers on twitter sending a pm . [ suspicious looking domain ] that was it . just a typo domain in the message screaming “ click through at your own risk. ” thnx <NUM> following me . be the ceo of you ! # supply another generic message . i ’ m not sure what action you want me to take with a statement like this . the hashtag is unrelated to their previous statement and they didn ’ t take the time to even spell out “ thank you. ” one thing is for sure , i am the ceo of me , and i ’ m in supply . i appreciate you as one of my recent follower . please rt if you like my thoughts . linkedin : [ removed ] and like my facebook page – there are more than a few things wrong with this dm . this person ’ s first mistake is starting with a version of “ thank you. ” you ’ re only adding insult to injury by thanking someone through an automatic message . it ’ s a waste of space and we end up with <NUM> of them in our inbox just saying “ thanks. ” – at least we ’ re heading in some semblance of the correct direction with this one . they ’ re giving us a legitimate action to take and asking to retweet them . your fans will retweet you if they want to , not because you asked using poor grammar . – it doesn ’ t make sense to randomly insert their linkedin profile and the demand to like their facebook page . apparently following them on twitter just wasn ’ t enough . if you ’ re using automatic messages as a sales funnel , don ’ t just direct someone to another social platform unless you ’ ve heavily monetized it . send them to sign up for your newsletter or give them a discount and push them to your web store instead . listen to the [ removed ] song on my sound cloud channel ? ? this one speaks for itself . while it might be advantageous for them to get more traffic to their song and channel for whatever reason , they failed to insert the entire name and author of the song , what their channel is , or even a link . if you ’ re going to request an action from a stranger in a dm , don ’ t make them do any research . hi edward zeiden ! thank you for following ! ! what are your hobbies or interests ? this message makes me think i accidentally signed up for a dating website . when you open your eyes to the magic of who you are , even in the blackness of night , … [ it goes on like this for <NUM> more lines ] this is definitely one of the more unique ones , but still unwarranted , long , and without a point . if you ’ re going to use an auto dm service , make sure your message is proofed for grammar and spelling , gives something back to the receiver as a true token of gratitude , and present a clear and concise action to take . here ’ s a decent example i found : get <NUM> % off your next order at [ removed ] with code tweet<NUM> at [ website ] . i ’ m not thrilled with receiving an ad i didn ’ t sign up for , but it ’ s a step in the right direction . if they also sent me a medium link to an educational article they wrote that aligned with my interests , that would soften the blow . remember that when you use an auto dm service , it ’ s usually to increase engagement on twitter , push users to your website to read your blog , take a survey , or buy something . if you ’ re not doing one of these , and in a quick and professional manner , it ’ s best to skip the message and try tweeting real content . source : http : <URL>/<NUM>-most-annoying-twitter-auto-dms/\n",
      "  scraped_at: <DATE> <NUM>:<NUM>:<NUM>\n",
      "  inserted_at: <DATE> <NUM>:<NUM>:<NUM>\n",
      "  updated_at: <DATE> <NUM>:<NUM>:<NUM>\n",
      "  title: <NUM> most annoying twitter auto dms\n",
      "  authors: morgan linton\n",
      "  keywords: \n",
      "  meta_keywords: [ `` ]\n",
      "  meta_description: \n",
      "  tags: \n",
      "  summary: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row_number, row in enumerate(subset_rows, start=start_row):\n",
    "    print(f\"Row {row_number}:\")\n",
    "    for column_name, cell_value in row.items():\n",
    "        tokens = word_tokenize(cell_value)\n",
    "        cleaned_tokens = [clean_text(token) for token in tokens]\n",
    "        cleaned_cell_value = ' '.join(cleaned_tokens)\n",
    "        print(f\"  {column_name}: {cleaned_cell_value}\")\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #2\n",
    "\n",
    "- Describe how you ended up representing the FakeNewsCorpus dataset (for instance with a Pandas dataframe). Argue for why you chose this design.\n",
    "- Did you discover any inherent problems with the data while working with it?\n",
    "- Report key properties of the data set - for instance through statistics or visualization.\n",
    "\n",
    "The exploration can include (but need not be limited to):\n",
    "\n",
    "- counting the number of URLs in the content\n",
    "- counting the number of dates in the content\n",
    "- counting the number of numeric values in the content\n",
    "- determining the 100 more frequent words that appear in the content\n",
    "- plot the frequency of the 10000 most frequent words (any interesting patterns?)\n",
    "- run the analysis in point 4 and 5 both before and after removing stopwords and applying stemming: do you see any difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Names:  ['', 'id', 'domain', 'type', 'url', 'content', 'scraped_at', 'inserted_at', 'updated_at', 'title', 'authors', 'keywords', 'meta_keywords', 'meta_description', 'tags', 'summary']\n"
     ]
    }
   ],
   "source": [
    "### Code ###\n",
    "#Find all the columns\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\"\n",
    "\n",
    "#Read CSV file from the url and parse it into a list of dictionaries\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    data = [row for row in csv.DictReader(response.read().decode(\"utf-8\").splitlines())]\n",
    "    \n",
    "print(\"Column Names: \", list(data[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of <NUM> tokens: 2487\n",
      "Number of <DATE> tokens: 40\n",
      "Number of <URL> tokens: 329\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import io\n",
    "import requests\n",
    "import re\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\"\n",
    "response = requests.get(url)\n",
    "content = response.content.decode(\"utf-8\")\n",
    "\n",
    "def count_tokens(rows):\n",
    "    num_count = 0\n",
    "    url_count = 0\n",
    "    date_count = 0\n",
    "    for row in rows:\n",
    "        content = row['content']\n",
    "        num_count += content.count(\"<NUM>\") #count number of \"<NUM>\" in column 'content'\n",
    "        date_count += content.count(\"<DATE>\") #count number of \"<DATE>\" in column 'content'\n",
    "        url_count += content.count(\"<URL>\") #count number of \"<URL>\" in column 'content'\n",
    "    return num_count, date_count, url_count\n",
    "\n",
    "rows = []\n",
    "for line in csv.DictReader(io.StringIO(content)):\n",
    "    line['content'] = clean_text(line['content'])\n",
    "    rows.append(line)\n",
    "\n",
    "num_count, date_count, url_count = count_tokens(rows)\n",
    "\n",
    "print(f\"Number of <NUM> tokens: {num_count}\")\n",
    "print(f\"Number of <DATE> tokens: {date_count}\")\n",
    "print(f\"Number of <URL> tokens: {url_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of <NUM> tokens: 2487\n"
     ]
    }
   ],
   "source": [
    "#Count number of \"<NUM>\" in column 'content'\n",
    "def count_num(rows):\n",
    "    num_count = 0\n",
    "    for row in rows:\n",
    "        content = row['content']\n",
    "        num_count += content.count(\"<NUM>\")\n",
    "    return num_count\n",
    "\n",
    "rows = []\n",
    "for line in csv.DictReader(io.StringIO(content)):\n",
    "    line['content'] = clean_text(line['content'])\n",
    "    rows.append(line)\n",
    "    \n",
    "print(f\"Number of <NUM> tokens: {num_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of <DATE> tokens: 40\n"
     ]
    }
   ],
   "source": [
    "def count_date(rows):\n",
    "    num_date = 0\n",
    "    for row in rows:\n",
    "        content = row['content']\n",
    "        num_date += content.count(\"<DATE>\")\n",
    "    return num_date\n",
    "\n",
    "rows = []\n",
    "for line in csv.DictReader(io.StringIO(content)):\n",
    "    line['content'] = clean_text(line['content'])\n",
    "    rows.append(line)\n",
    "    \n",
    "\n",
    "print(f\"Number of <DATE> tokens: {date_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of <URL> tokens: 329\n"
     ]
    }
   ],
   "source": [
    "def count_url(rows):\n",
    "    num_url = 0\n",
    "    for row in rows:\n",
    "        content = row['content']\n",
    "        num_url += content.count(\"<URL>\")\n",
    "    return num_url\n",
    "\n",
    "rows = []\n",
    "for line in csv.DictReader(io.StringIO(content)):\n",
    "    line['content'] = clean_text(line['content'])\n",
    "    rows.append(line)\n",
    "    \n",
    "print(f\"Number of <URL> tokens: {url_count}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #3\n",
    "\n",
    "Apply your data preprocessing pipeline to a larger proportion of the FakeNewsCorpus https://github.com/several27/FakeNewsCorpus/releases/tag/v1.0\n",
    "\n",
    "You may find it challenging to run your data processing pipeline on the entire FakeNewsCorpus. At a minimum, you should be able to process 10% of the data using your pipeline,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #4\n",
    "\n",
    "Split the resulting dataset into a training, validation, and test splits. A common strategy is to uniformly at random split the data 80% / 10% / 10%. You will use the training data to train your baseline and advanced models, the validation data can be used for model selection and hyperparameter tuning, while the test data should only be used in Part 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
