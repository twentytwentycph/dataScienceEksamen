{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake News Project\n",
    "\n",
    "By: Mateo Anusic, Emil Thorlund, Lucas A. Rosing, Victor Bergien"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #1\n",
    "\n",
    "- Structure, process and clean the text.\n",
    "- Tokenize the text\n",
    "- Remove stopwords and compute the size of the vocabulary.\n",
    "- Compute the reduction rate of the vocabulary size after removing stopwords.\n",
    "- Remove word variations with stemming and compute the size of the vocabulary.\n",
    "- Compute the reduction rate of the vocabulary size after stemming.\n",
    "\n",
    "Describe which procedures (and which libraries) you used and why they are appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "# Note that some may be missing, you can install them using pip\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from cleantext import clean \n",
    "import cleantext\n",
    "import csv\n",
    "import requests\n",
    "from io import StringIO\n",
    "from itertools import islice\n",
    "\n",
    "# Download the dataset from the following link\n",
    "data_url = 'https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv'\n",
    "\n",
    "# Download the nltk data for the first time\n",
    "#nltk.download('punkt')\n",
    "\n",
    "# Read the dataset\n",
    "response = requests.get(data_url)\n",
    "response.raise_for_status()  #Raise exeption\n",
    "\n",
    "# Convert the dataset into a csv file\n",
    "csv_data = response.content.decode('utf-8')\n",
    "csv_file = StringIO(csv_data)\n",
    "\n",
    "# Read the csv file\n",
    "reader = csv.DictReader(csv_file)\n",
    "\n",
    "# Create a subset of the dataset for testing\n",
    "start_row = 100\n",
    "end_row = 102\n",
    "\n",
    "subset_rows = list(islice(reader, start_row, end_row))\n",
    "\n",
    "#for row_number, row in enumerate(subset_rows, start=start_row):\n",
    "#    print(f\"Row {row_number}:\")\n",
    "#    for column_name, cell_value in row.items():\n",
    "#        print(f\"  {column_name}: {cell_value}\")\n",
    "#    print()  # Print an empty line to separate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expressions for identifying dates, numbers, emails and urls\n",
    "date_pattern = re.compile(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}.\\d{6})|'              # YYYY-MM-DD HH:MM:SS.MMMMMM\n",
    "                        r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})|'                      # YYYY-MM-DD HH:MM:SS\n",
    "                        r'(\\d{4}-\\d{2}-\\d{2})|'                                        # YYYY-MM-DD\n",
    "                        r'(\\d{4}\\.\\d{2}\\.\\d{2})|'                                      # YYYY.MM.DD \n",
    "                        r'(\\d{2}\\.\\d{2}\\.\\d{4})|'                                      # DD.MM.YYYY\n",
    "                        r'(\\d{4}/\\d{2}/\\d{2})|'                                        # YYYY/MM/DD\n",
    "                        r'(\\d{2}/\\d{2}/\\d{4})|'                                        # DD/MM/YYYY\n",
    "                        r'((january|february|march|april|june|july|august|september|'  # <Month> DD YYYY\n",
    "                        r'october|november|december) \\d{2}, \\d{4})', re.IGNORECASE)  \n",
    "number_pattern = re.compile(r'(\\d+(?:,\\d{3})*(?:\\.\\d+)?)')\n",
    "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+|\\S+\\.com')\n",
    "\n",
    "# A function to clean the text and tokenize it\n",
    "# Using regular expressions to identify dates, numbers, emails and urls\n",
    "# And replacing them with <DATE>, <NUM>, <EMAIL> and <URL> respectively\n",
    "# Then tokenizing the text using nltk.word_tokenize\n",
    "def clean_text_and_tokenize(read):\n",
    "    if not isinstance(read, str):\n",
    "        return []\n",
    "    read = read.lower() # Convert the text to lowercase\n",
    "    read = re.sub(r\"\\s+\", \" \", read) # Remove extra spaces\n",
    "    read = re.sub(date_pattern, '<DATE>', read) # Replace dates with <DATE>\n",
    "    read = re.sub(number_pattern, \"<NUM>\", read) # Replace numbers with <NUM>\n",
    "    read = re.sub(r\"\\S+@\\S+\", \"<EMAIL>\", read) # Replace emails with <EMAIL>\n",
    "    read = re.sub(url_pattern, \"<URL>\", read) # Replace urls with <URL>\n",
    "    tokens = word_tokenize(read)  # Tokenize the text\n",
    "    return tokens\n",
    "\n",
    "# A function to remove stopwords using nltk.corpus.stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    # Create a set of stopwords from nltk.corpus.stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Remove the stopwords from the tokens and return the filtered tokens\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "# A function to stem the tokens using nltk.stem.PorterStemmer\n",
    "def stem_tokens(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    # Stem the tokens and return the stemmed tokens\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 100:\n",
      "  : < num >\n",
      "  id: < num >\n",
      "  domain: < url >\n",
      "  type: fake\n",
      "  url: < url >\n",
      "  content: greenmedinfo – action item link % reader think stori fact . add two cent . headlin : bitcoin & blockchain search exceed trump ! blockchain stock next ! one link greedmedinfo updat incomplet . letter write campaign locat : make fda advisori , mandatori sourc : < url >\n",
      "  scraped_at: < date >\n",
      "  inserted_at: < date >\n",
      "  updated_at: < date >\n",
      "  title: greenmedinfo – action item link\n",
      "  authors: downsiz dc\n",
      "  keywords: \n",
      "  meta_keywords: [ `` ]\n",
      "  meta_description: \n",
      "  tags: \n",
      "  summary: \n",
      "\n",
      "Row 101:\n",
      "  : < num >\n",
      "  id: < num >\n",
      "  domain: < url >\n",
      "  type: fake\n",
      "  url: < url >\n",
      "  content: < num > annoy twitter auto dm headlin : bitcoin & blockchain search exceed trump ! blockchain stock next ! seen “ cheap supplement < num > ” “ regist busi program ’ receiv endless benefits. ” like thought email spam . ’ real exampl spam , twitter . last week wrote < num > worst social media mistak . one mistak annoy auto direct messag . may receiv one engag someon ’ account follow like tweet . immedi click “ follow ” , program automat send pre-written messag . ’ entir idea auto dm servic , ’ gotten complet hand mostli use irrespons . end result inbox satur pointless messag , forc authent one-on-on interact bottom . worst seven messag ’ receiv : hola . gracia por seguirm . deja saber si hay algo en que lo puedo ayudar innovar ! ’ go send anyon messag , make sure ’ person ’ nativ speak languag . even translat , messag standard “ thank , let know help. ” ’ sure mani peopl ask help stranger twitter send pm . [ suspici look domain ] . typo domain messag scream “ click risk. ” thnx < num > follow . ceo ! # suppli anoth gener messag . ’ sure action want take statement like . hashtag unrel previou statement ’ take time even spell “ thank you. ” one thing sure , ceo , ’ suppli . appreci one recent follow . pleas rt like thought . linkedin : [ remov ] like facebook page – thing wrong dm . person ’ first mistak start version “ thank you. ” ’ ad insult injuri thank someon automat messag . ’ wast space end < num > inbox say “ thanks. ” – least ’ head semblanc correct direct one . ’ give us legitim action take ask retweet . fan retweet want , ask use poor grammar . – ’ make sens randomli insert linkedin profil demand like facebook page . appar follow twitter ’ enough . ’ use automat messag sale funnel , ’ direct someon anoth social platform unless ’ heavili monet . send sign newslett give discount push web store instead . listen [ remov ] song sound cloud channel ? ? one speak . might advantag get traffic song channel whatev reason , fail insert entir name author song , channel , even link . ’ go request action stranger dm , ’ make research . hi edward zeiden ! thank follow ! ! hobbi interest ? messag make think accident sign date websit . open eye magic , even black night , … [ goe like < num > line ] definit one uniqu one , still unwarr , long , without point . ’ go use auto dm servic , make sure messag proof grammar spell , give someth back receiv true token gratitud , present clear concis action take . ’ decent exampl found : get < num > % next order [ remov ] code tweet < num > [ websit ] . ’ thrill receiv ad ’ sign , ’ step right direct . also sent medium link educ articl wrote align interest , would soften blow . rememb use auto dm servic , ’ usual increas engag twitter , push user websit read blog , take survey , buy someth . ’ one , quick profession manner , ’ best skip messag tri tweet real content . sourc : < url >\n",
      "  scraped_at: < date >\n",
      "  inserted_at: < date >\n",
      "  updated_at: < date >\n",
      "  title: < num > annoy twitter auto dm\n",
      "  authors: morgan linton\n",
      "  keywords: \n",
      "  meta_keywords: [ `` ]\n",
      "  meta_description: \n",
      "  tags: \n",
      "  summary: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the cleaned text and tokens for testing\n",
    "for row_number, row in enumerate(subset_rows, start=start_row):\n",
    "    print(f\"Row {row_number}:\") # Print the row number\n",
    "    for column_name, cell_value in row.items():\n",
    "        # For each column, print the column name and the cleaned text and tokens\n",
    "        tokens = clean_text_and_tokenize(cell_value)  # Clean text and tokenize\n",
    "        no_stopwords_tokens = remove_stopwords(tokens)\n",
    "        stemmed_tokens = stem_tokens(no_stopwords_tokens)\n",
    "        cleaned_cell_value = ' '.join(stemmed_tokens)\n",
    "        print(f\"  {column_name}: {cleaned_cell_value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'an', 'example', 'sentence', '.']\n",
      "['example', 'sentence', '.']\n",
      "['exampl', 'sentenc', '.']\n"
     ]
    }
   ],
   "source": [
    "# Testing the functions on a sample text\n",
    "test_line = \"This is an example sentence.\"\n",
    "print(clean_text_and_tokenize(test_line))\n",
    "print(remove_stopwords(clean_text_and_tokenize(test_line)))\n",
    "print(stem_tokens(remove_stopwords(clean_text_and_tokenize(test_line))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Unique words in the data before and after preprocessing ###\n",
      "\n",
      "Unique words before preprocessing:            19300\n",
      "\n",
      "Unique words after removing stopwords:        17738\n",
      "\n",
      "Reduction rate in % after removing stopwords: 8.09%\n",
      "\n",
      "Unique words after stemming:                  12636\n",
      "\n",
      "Reduction rate in % after stemming:           34.03%\n"
     ]
    }
   ],
   "source": [
    "# A function to count the number of unique words in a list of documents\n",
    "def count_unique_words(documents):\n",
    "    # Create a set of unique words from the list of documents\n",
    "    word_set = set()\n",
    "    for document in documents:\n",
    "        # Clean the text and tokenize it\n",
    "        tokens = clean_text_and_tokenize(document)  \n",
    "        word_set.update(tokens)\n",
    "    # Return the number of unique words\n",
    "    return len(word_set)\n",
    "\n",
    "# Count unique words before removing stopwords and stemming\n",
    "unique_words = set(word_tokenize(csv_data.lower()))\n",
    "\n",
    "# Count unique words after removing stopwords\n",
    "no_stopwords_unique_words = remove_stopwords(unique_words)\n",
    "unique_words_after_stopwords = count_unique_words(no_stopwords_unique_words)\n",
    "\n",
    "#Compute reduction rate in % after removing stopwords\n",
    "reduction_rate= ((len(unique_words) - unique_words_after_stopwords) / len(unique_words)) * 100\n",
    "\n",
    "#Count unique words after stemming\n",
    "after_stemming = stem_tokens(no_stopwords_unique_words)\n",
    "unique_words_after_stemming = count_unique_words(after_stemming)\n",
    "\n",
    "#Compute reduction rate in % after stemming the amount of words after removed stopwords\n",
    "reduction_Rate = ((len(after_stemming) - unique_words_after_stemming) / len(after_stemming)) * 100\n",
    "\n",
    "# Print the number of unique words before and after preprocessing\n",
    "print(\"### Unique words in the data before and after preprocessing ###\")\n",
    "print( ) \n",
    "print(f\"Unique words before preprocessing:            {len(unique_words)}\")\n",
    "print( ) \n",
    "print(f\"Unique words after removing stopwords:        {unique_words_after_stopwords}\")\n",
    "print( )\n",
    "print(f\"Reduction rate in % after removing stopwords: {reduction_rate:.2f}%\")\n",
    "print( )\n",
    "print(f\"Unique words after stemming:                  {unique_words_after_stemming}\")\n",
    "print( )\n",
    "print(f\"Reduction rate in % after stemming:           {reduction_Rate:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #2\n",
    "\n",
    "- Describe how you ended up representing the FakeNewsCorpus dataset (for instance with a Pandas dataframe). Argue for why you chose this design.\n",
    "- Did you discover any inherent problems with the data while working with it?\n",
    "- Report key properties of the data set - for instance through statistics or visualization.\n",
    "\n",
    "The exploration can include (but need not be limited to):\n",
    "\n",
    "- counting the number of URLs in the content\n",
    "- counting the number of dates in the content\n",
    "- counting the number of numeric values in the content\n",
    "- determining the 100 more frequent words that appear in the content\n",
    "- plot the frequency of the 10000 most frequent words (any interesting patterns?)\n",
    "- run the analysis in point 4 and 5 both before and after removing stopwords and applying stemming: do you see any difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Names:  ['', 'id', 'domain', 'type', 'url', 'content', 'scraped_at', 'inserted_at', 'updated_at', 'title', 'authors', 'keywords', 'meta_keywords', 'meta_description', 'tags', 'summary']\n"
     ]
    }
   ],
   "source": [
    "#Find all the columns\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "# The url of the csv file\n",
    "url = \"https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\"\n",
    "\n",
    "#Read CSV file from the url and parse it into a list of dictionaries\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    data = [row for row in csv.DictReader(response.read().decode(\"utf-8\").splitlines())]\n",
    "\n",
    "# Print the column names for identification    \n",
    "print(\"Column Names: \", list(data[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of <NUM> tokens: 2487\n",
      "Number of <DATE> tokens: 40\n",
      "Number of <URL> tokens: 329\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import io\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# The url of the csv file which is then read and stored in content\n",
    "url = \"https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\"\n",
    "response = requests.get(url)\n",
    "content = response.content.decode(\"utf-8\")\n",
    "\n",
    "# A to count the number of tokens in the column 'content'\n",
    "def count_tokens(rows):\n",
    "    # Creates variables in which to store the number of tokens\n",
    "    num_count = 0\n",
    "    url_count = 0\n",
    "    date_count = 0\n",
    "    # Iterate over the rows and count the number of tokens\n",
    "    for row in rows:\n",
    "        content = row['content']\n",
    "        num_count += content.count(\"NUM\") #count number of \"<NUM>\" in column 'content'\n",
    "        date_count += content.count(\"DATE\") #count number of \"<DATE>\" in column 'content'\n",
    "        url_count += content.count(\"URL\") #count number of \"<URL>\" in column 'content'\n",
    "    return num_count, date_count, url_count\n",
    "\n",
    "rows = []\n",
    "for line in csv.DictReader(io.StringIO(content)):\n",
    "    line['content'] = clean_text_and_tokenize(line['content'])\n",
    "    rows.append(line)\n",
    "\n",
    "# Count the number of tokens in the column 'content'\n",
    "num_count, date_count, url_count = count_tokens(rows)\n",
    "\n",
    "# Print the number of tokens in the column 'content'\n",
    "print(f\"Number of <NUM> tokens: {num_count}\")\n",
    "print(f\"Number of <DATE> tokens: {date_count}\")\n",
    "print(f\"Number of <URL> tokens: {url_count}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #3\n",
    "\n",
    "Apply your data preprocessing pipeline to a larger proportion of the FakeNewsCorpus https://github.com/several27/FakeNewsCorpus/releases/tag/v1.0\n",
    "\n",
    "You may find it challenging to run your data processing pipeline on the entire FakeNewsCorpus. At a minimum, you should be able to process 10% of the data using your pipeline,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "### Untested Code - Fixed a previous error ###\n",
    "\n",
    "# import sys\n",
    "# \n",
    "# # Set the field size limit to the maximum allowed value\n",
    "# max_int = sys.maxsize\n",
    "# while True:\n",
    "#     try:\n",
    "#         csv.field_size_limit(max_int)\n",
    "#         break\n",
    "#     except OverflowError:\n",
    "#         max_int = max_int // 2\n",
    "\n",
    "def process_text(text):\n",
    "    if text is None:\n",
    "        return ''\n",
    "    tokens = clean_text_and_tokenize(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    stemmed_tokens = stem_tokens(tokens)\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "columns_to_process = {'content', 'type', 'meta_description', 'domain', 'title', 'meta_keyboards'}\n",
    "with open('news_cleaned_2018_02_13.csv', encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "\n",
    "    with open('news_cleaned_2018_02_13-results3.csv', 'w', encoding=\"utf-8\") as fOut:\n",
    "        fieldnames = reader.fieldnames + [\"processed_text\"]\n",
    "        writer = csv.DictWriter(fOut, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in reader:\n",
    "            processed_row = {column_name: (process_text(cell_value) if column_name in columns_to_process else cell_value) for column_name, cell_value in row.items()}\n",
    "            writer.writerow(processed_row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #4\n",
    "\n",
    "Split the resulting dataset into a training, validation, and test splits. A common strategy is to uniformly at random split the data 80% / 10% / 10%. You will use the training data to train your baseline and advanced models, the validation data can be used for model selection and hyperparameter tuning, while the test data should only be used in Part 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "# pip install dask-ml\n",
    "import dask.dataframe as dd\n",
    "import dask_ml.model_selection\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a Dask DataFrame with specified data types\n",
    "# These dtypes can be found by printing the dask dataframe\n",
    "cleaned_data = dd.read_csv('news_cleaned_2018_02_13-results.csv', encoding=\"utf-8\", dtype={\n",
    "        'Unnamed: 0': 'object',\n",
    "        'id': 'object',\n",
    "        'domain': 'object',\n",
    "        'type': 'object',\n",
    "        'url': 'object',\n",
    "        'content': 'object',\n",
    "        'scraped_at': 'object',\n",
    "        'inserted_at': 'object',\n",
    "        'updated_at': 'object',\n",
    "        'title': 'object',\n",
    "        'authors': 'object',\n",
    "        'keywords': 'float64',\n",
    "        'meta_keywords': 'object',\n",
    "        'meta_description': 'object',\n",
    "        'tags': 'object',\n",
    "        'summary': 'float64',\n",
    "        'tokens': 'object',\n",
    "        'filtered_tokens': 'object',\n",
    "        'stemmed_tokens': 'object',\n",
    "    },)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to modify the 'type' column values\n",
    "# This function is used to simplify the classification problem\n",
    "# The 'reliable' and 'political' types are combined into a single 'reliable' type\n",
    "def modify_type(x):\n",
    "    if x == 'reliabl' or x == 'polit':\n",
    "        return 'reliable'\n",
    "    else:\n",
    "        return 'fake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the modify_type function to the 'type' column using the map function\n",
    "cleaned_data['type'] = cleaned_data['type'].map(modify_type, meta=('type', 'object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify if there are any missing values in the 'type' column\n",
    "# If so, they are replaced with an empty string\n",
    "def nan_to_empty(x):\n",
    "    if isinstance(x, float) and np.isnan(x):\n",
    "        return ''\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the nan_to_empty function to the 'type' column using the map function\n",
    "cleaned_data['content'] = cleaned_data['content'].map(nan_to_empty, meta=('content', 'object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y as the 'content' and 'type' columns\n",
    "y = cleaned_data['type']\n",
    "X = cleaned_data['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "# In an 80/10/10 split\n",
    "\n",
    "X_train, X_test, y_train, y_test = dask_ml.model_selection.train_test_split(X, y, test_size=0.2, random_state=0, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = dask_ml.model_selection.train_test_split(X_train, y_train, test_size=0.125, random_state=0, shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
