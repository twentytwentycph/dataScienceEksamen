{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake News Project\n",
    "\n",
    "By: Mateo Anusic, Emil Thorlund, Lucas A. Rosing, Victor Bergien"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #1\n",
    "\n",
    "- Tokenize the text\n",
    "- Remove stopwords and compute the size of the vocabulary.\n",
    "- Compute the reduction rate of the vocabulary size after removing stopwords.\n",
    "- Remove word variations with stemming and compute the size of the vocabulary.\n",
    "- Compute the reduction rate of the vocabulary size after stemming.\n",
    "\n",
    "Describe which procedures (and which libraries) you used and why they are appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code ###\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from cleantext import clean \n",
    "import cleantext\n",
    "import csv\n",
    "import requests\n",
    "from io import StringIO\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "data_url = 'https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv'\n",
    "\n",
    "#nltk.download('punkt')\n",
    "\n",
    "response = requests.get(data_url)\n",
    "response.raise_for_status()  #Raise exeption\n",
    "\n",
    "csv_data = response.content.decode('utf-8')\n",
    "csv_file = StringIO(csv_data)\n",
    "\n",
    "reader = csv.DictReader(csv_file)\n",
    "\n",
    "start_row = 100\n",
    "end_row = 102\n",
    "\n",
    "subset_rows = list(islice(reader, start_row, end_row))\n",
    "\n",
    "#for row_number, row in enumerate(subset_rows, start=start_row):\n",
    "#    print(f\"Row {row_number}:\")\n",
    "#    for column_name, cell_value in row.items():\n",
    "#        print(f\"  {column_name}: {cell_value}\")\n",
    "#    print()  # Print an empty line to separate rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_pattern = re.compile(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}.\\d{6})|'            # YYYY-MM-DD HH:MM:SS.MMMMMM\n",
    "                        r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})|'                      # YYYY-MM-DD HH:MM:SS\n",
    "                        r'(\\d{4}-\\d{2}-\\d{2})|'                                        # YYYY-MM-DD\n",
    "                        r'(\\d{4}\\.\\d{2}\\.\\d{2})|'                                      # YYYY.MM.DD \n",
    "                        r'(\\d{2}\\.\\d{2}\\.\\d{4})|'                                      # DD.MM.YYYY\n",
    "                        r'(\\d{4}/\\d{2}/\\d{2})|'                                        # YYYY/MM/DD\n",
    "                        r'(\\d{2}/\\d{2}/\\d{4})|'                                        # DD/MM/YYYY\n",
    "                        r'((january|february|march|april|june|july|august|september|'  # <Month> DD YYYY\n",
    "                        r'october|november|december) \\d{2}, \\d{4})', re.IGNORECASE)  \n",
    "number_pattern = re.compile(r'(\\d+(?:,\\d{3})*(?:\\.\\d+)?)')\n",
    "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+|\\S+\\.com')\n",
    "\n",
    "def clean_text(read):\n",
    "    read = read.lower()\n",
    "    read = re.sub(r\"\\s+\", \" \", read)\n",
    "    read = re.sub(date_pattern, '<DATE>', read)\n",
    "    read = re.sub(number_pattern, \"<NUM>\", read)\n",
    "    read = re.sub(r\"\\S+@\\S+\", \"<EMAIL>\", read)\n",
    "    read = re.sub(url_pattern, \"<URL>\", read)\n",
    "    return read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row_number, row in enumerate(subset_rows, start=start_row):\n",
    "    print(f\"Row {row_number}:\")\n",
    "    for column_name, cell_value in row.items():\n",
    "        tokens = word_tokenize(cell_value)\n",
    "        cleaned_tokens = [clean_text(token) for token in tokens]\n",
    "        cleaned_cell_value = ' '.join(cleaned_tokens)\n",
    "        print(f\"  {column_name}: {cleaned_cell_value}\")\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #2\n",
    "\n",
    "- Describe how you ended up representing the FakeNewsCorpus dataset (for instance with a Pandas dataframe). Argue for why you chose this design.\n",
    "- Did you discover any inherent problems with the data while working with it?\n",
    "- Report key properties of the data set - for instance through statistics or visualization.\n",
    "\n",
    "The exploration can include (but need not be limited to):\n",
    "\n",
    "- counting the number of URLs in the content\n",
    "- counting the number of dates in the content\n",
    "- counting the number of numeric values in the content\n",
    "- determining the 100 more frequent words that appear in the content\n",
    "- plot the frequency of the 10000 most frequent words (any interesting patterns?)\n",
    "- run the analysis in point 4 and 5 both before and after removing stopwords and applying stemming: do you see any difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code ###\n",
    "#Find all the columns\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\"\n",
    "\n",
    "#Read CSV file from the url and parse it into a list of dictionaries\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    data = [row for row in csv.DictReader(response.read().decode(\"utf-8\").sp)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #3\n",
    "\n",
    "Apply your data preprocessing pipeline to a larger proportion of the FakeNewsCorpus https://github.com/several27/FakeNewsCorpus/releases/tag/v1.0\n",
    "\n",
    "You may find it challenging to run your data processing pipeline on the entire FakeNewsCorpus. At a minimum, you should be able to process 10% of the data using your pipeline,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #4\n",
    "\n",
    "Split the resulting dataset into a training, validation, and test splits. A common strategy is to uniformly at random split the data 80% / 10% / 10%. You will use the training data to train your baseline and advanced models, the validation data can be used for model selection and hyperparameter tuning, while the test data should only be used in Part 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code ###"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
