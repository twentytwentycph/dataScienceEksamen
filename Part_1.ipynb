{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake News Project\n",
    "\n",
    "By: Mateo Anusic, Emil Thorlund, Lucas A. Rosing, Victor Bergien"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #1\n",
    "\n",
    "- Structure, process and clean the text.\n",
    "- Tokenize the text\n",
    "- Remove stopwords and compute the size of the vocabulary.\n",
    "- Compute the reduction rate of the vocabulary size after removing stopwords.\n",
    "- Remove word variations with stemming and compute the size of the vocabulary.\n",
    "- Compute the reduction rate of the vocabulary size after stemming.\n",
    "\n",
    "Describe which procedures (and which libraries) you used and why they are appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code ###\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from cleantext import clean \n",
    "import cleantext\n",
    "import csv\n",
    "import requests\n",
    "from io import StringIO\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "data_url = 'https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv'\n",
    "\n",
    "#nltk.download('punkt')\n",
    "\n",
    "response = requests.get(data_url)\n",
    "response.raise_for_status()  #Raise exeption\n",
    "\n",
    "csv_data = response.content.decode('utf-8')\n",
    "csv_file = StringIO(csv_data)\n",
    "\n",
    "reader = csv.DictReader(csv_file)\n",
    "\n",
    "start_row = 100\n",
    "end_row = 102\n",
    "\n",
    "subset_rows = list(islice(reader, start_row, end_row))\n",
    "\n",
    "#for row_number, row in enumerate(subset_rows, start=start_row):\n",
    "#    print(f\"Row {row_number}:\")\n",
    "#    for column_name, cell_value in row.items():\n",
    "#        print(f\"  {column_name}: {cell_value}\")\n",
    "#    print()  # Print an empty line to separate rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_pattern = re.compile(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}.\\d{6})|'            # YYYY-MM-DD HH:MM:SS.MMMMMM\n",
    "                        r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})|'                      # YYYY-MM-DD HH:MM:SS\n",
    "                        r'(\\d{4}-\\d{2}-\\d{2})|'                                        # YYYY-MM-DD\n",
    "                        r'(\\d{4}\\.\\d{2}\\.\\d{2})|'                                      # YYYY.MM.DD \n",
    "                        r'(\\d{2}\\.\\d{2}\\.\\d{4})|'                                      # DD.MM.YYYY\n",
    "                        r'(\\d{4}/\\d{2}/\\d{2})|'                                        # YYYY/MM/DD\n",
    "                        r'(\\d{2}/\\d{2}/\\d{4})|'                                        # DD/MM/YYYY\n",
    "                        r'((january|february|march|april|june|july|august|september|'  # <Month> DD YYYY\n",
    "                        r'october|november|december) \\d{2}, \\d{4})', re.IGNORECASE)  \n",
    "number_pattern = re.compile(r'(\\d+(?:,\\d{3})*(?:\\.\\d+)?)')\n",
    "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+|\\S+\\.com')\n",
    "\n",
    "def clean_text_and_tokenize(read):\n",
    "    read = read.lower()\n",
    "    read = re.sub(r\"\\s+\", \" \", read)\n",
    "    read = re.sub(date_pattern, '<DATE>', read)\n",
    "    read = re.sub(number_pattern, \"<NUM>\", read)\n",
    "    read = re.sub(r\"\\S+@\\S+\", \"<EMAIL>\", read)\n",
    "    read = re.sub(url_pattern, \"<URL>\", read)\n",
    "    tokens = word_tokenize(read)  # Tokenize the text\n",
    "    return tokens\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 100:\n",
      "  : < num >\n",
      "  id: < num >\n",
      "  domain: < url >\n",
      "  type: fake\n",
      "  url: < url >\n",
      "  content: greenmedinfo – action item link % reader think stori fact . add two cent . headlin : bitcoin & blockchain search exceed trump ! blockchain stock next ! one link greedmedinfo updat incomplet . letter write campaign locat : make fda advisori , mandatori sourc : < url >\n",
      "  scraped_at: < date >\n",
      "  inserted_at: < date >\n",
      "  updated_at: < date >\n",
      "  title: greenmedinfo – action item link\n",
      "  authors: downsiz dc\n",
      "  keywords: \n",
      "  meta_keywords: [ `` ]\n",
      "  meta_description: \n",
      "  tags: \n",
      "  summary: \n",
      "\n",
      "Row 101:\n",
      "  : < num >\n",
      "  id: < num >\n",
      "  domain: < url >\n",
      "  type: fake\n",
      "  url: < url >\n",
      "  content: < num > annoy twitter auto dm headlin : bitcoin & blockchain search exceed trump ! blockchain stock next ! seen “ cheap supplement < num > ” “ regist busi program ’ receiv endless benefits. ” like thought email spam . ’ real exampl spam , twitter . last week wrote < num > worst social media mistak . one mistak annoy auto direct messag . may receiv one engag someon ’ account follow like tweet . immedi click “ follow ” , program automat send pre-written messag . ’ entir idea auto dm servic , ’ gotten complet hand mostli use irrespons . end result inbox satur pointless messag , forc authent one-on-on interact bottom . worst seven messag ’ receiv : hola . gracia por seguirm . deja saber si hay algo en que lo puedo ayudar innovar ! ’ go send anyon messag , make sure ’ person ’ nativ speak languag . even translat , messag standard “ thank , let know help. ” ’ sure mani peopl ask help stranger twitter send pm . [ suspici look domain ] . typo domain messag scream “ click risk. ” thnx < num > follow . ceo ! # suppli anoth gener messag . ’ sure action want take statement like . hashtag unrel previou statement ’ take time even spell “ thank you. ” one thing sure , ceo , ’ suppli . appreci one recent follow . pleas rt like thought . linkedin : [ remov ] like facebook page – thing wrong dm . person ’ first mistak start version “ thank you. ” ’ ad insult injuri thank someon automat messag . ’ wast space end < num > inbox say “ thanks. ” – least ’ head semblanc correct direct one . ’ give us legitim action take ask retweet . fan retweet want , ask use poor grammar . – ’ make sens randomli insert linkedin profil demand like facebook page . appar follow twitter ’ enough . ’ use automat messag sale funnel , ’ direct someon anoth social platform unless ’ heavili monet . send sign newslett give discount push web store instead . listen [ remov ] song sound cloud channel ? ? one speak . might advantag get traffic song channel whatev reason , fail insert entir name author song , channel , even link . ’ go request action stranger dm , ’ make research . hi edward zeiden ! thank follow ! ! hobbi interest ? messag make think accident sign date websit . open eye magic , even black night , … [ goe like < num > line ] definit one uniqu one , still unwarr , long , without point . ’ go use auto dm servic , make sure messag proof grammar spell , give someth back receiv true token gratitud , present clear concis action take . ’ decent exampl found : get < num > % next order [ remov ] code tweet < num > [ websit ] . ’ thrill receiv ad ’ sign , ’ step right direct . also sent medium link educ articl wrote align interest , would soften blow . rememb use auto dm servic , ’ usual increas engag twitter , push user websit read blog , take survey , buy someth . ’ one , quick profession manner , ’ best skip messag tri tweet real content . sourc : < url >\n",
      "  scraped_at: < date >\n",
      "  inserted_at: < date >\n",
      "  updated_at: < date >\n",
      "  title: < num > annoy twitter auto dm\n",
      "  authors: morgan linton\n",
      "  keywords: \n",
      "  meta_keywords: [ `` ]\n",
      "  meta_description: \n",
      "  tags: \n",
      "  summary: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row_number, row in enumerate(subset_rows, start=start_row):\n",
    "    print(f\"Row {row_number}:\")\n",
    "    for column_name, cell_value in row.items():\n",
    "        tokens = clean_text_and_tokenize(cell_value)  # Clean text and tokenize\n",
    "        no_stopwords_tokens = remove_stopwords(tokens)\n",
    "        stemmed_tokens = stem_tokens(no_stopwords_tokens)\n",
    "        cleaned_cell_value = ' '.join(stemmed_tokens)\n",
    "        print(f\"  {column_name}: {cleaned_cell_value}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'an', 'example', 'sentence', '.']\n",
      "['example', 'sentence', '.']\n",
      "['exampl', 'sentenc', '.']\n"
     ]
    }
   ],
   "source": [
    "test_line = \"This is an example sentence.\"\n",
    "print(clean_text_and_tokenize(test_line))\n",
    "print(remove_stopwords(clean_text_and_tokenize(test_line)))\n",
    "print(stem_tokens(remove_stopwords(clean_text_and_tokenize(test_line))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Unique words in the data before and after preprocessing ###\n",
      "\n",
      "Unique words before preprocessing:            19293\n",
      "\n",
      "Unique words after removing stopwords:        17737\n",
      "\n",
      "Reduction rate in % after removing stopwords: 8.07%\n",
      "\n",
      "Unique words after stemming:                  12632\n",
      "\n",
      "Reduction rate in % after stemming:           34.03%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def count_unique_words(documents):\n",
    "    word_set = set()\n",
    "    for document in documents:\n",
    "        tokens = clean_text_and_tokenize(document)  \n",
    "        word_set.update(tokens)\n",
    "    return len(word_set)\n",
    "\n",
    "# Count unique words before removing stopwords and stemming\n",
    "unique_words = set(word_tokenize(csv_data.lower()))\n",
    "\n",
    "# Count unique words after removing stopwords\n",
    "no_stopwords_unique_words = remove_stopwords(unique_words)\n",
    "unique_words_after_stopwords = count_unique_words(no_stopwords_unique_words)\n",
    "\n",
    "#Compute reduction rate in % after removing stopwords\n",
    "reduction_rate= ((len(unique_words) - unique_words_after_stopwords) / len(unique_words)) * 100\n",
    "\n",
    "#Count unique words after stemming\n",
    "after_stemming = stem_tokens(no_stopwords_unique_words)\n",
    "unique_words_after_stemming = count_unique_words(after_stemming)\n",
    "\n",
    "#Compute reduction rate in % after stemming the amount of words after removed stopwords\n",
    "reduction_Rate = ((len(after_stemming) - unique_words_after_stemming) / len(after_stemming)) * 100\n",
    "\n",
    "print(\"### Unique words in the data before and after preprocessing ###\")\n",
    "print( ) \n",
    "print(f\"Unique words before preprocessing:            {len(unique_words)}\")\n",
    "print( ) \n",
    "print(f\"Unique words after removing stopwords:        {unique_words_after_stopwords}\")\n",
    "print( )\n",
    "print(f\"Reduction rate in % after removing stopwords: {reduction_rate:.2f}%\")\n",
    "print( )\n",
    "print(f\"Unique words after stemming:                  {unique_words_after_stemming}\")\n",
    "print( )\n",
    "print(f\"Reduction rate in % after stemming:           {reduction_Rate:.2f}%\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #2\n",
    "\n",
    "- Describe how you ended up representing the FakeNewsCorpus dataset (for instance with a Pandas dataframe). Argue for why you chose this design.\n",
    "- Did you discover any inherent problems with the data while working with it?\n",
    "- Report key properties of the data set - for instance through statistics or visualization.\n",
    "\n",
    "The exploration can include (but need not be limited to):\n",
    "\n",
    "- counting the number of URLs in the content\n",
    "- counting the number of dates in the content\n",
    "- counting the number of numeric values in the content\n",
    "- determining the 100 more frequent words that appear in the content\n",
    "- plot the frequency of the 10000 most frequent words (any interesting patterns?)\n",
    "- run the analysis in point 4 and 5 both before and after removing stopwords and applying stemming: do you see any difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Names:  ['', 'id', 'domain', 'type', 'url', 'content', 'scraped_at', 'inserted_at', 'updated_at', 'title', 'authors', 'keywords', 'meta_keywords', 'meta_description', 'tags', 'summary']\n"
     ]
    }
   ],
   "source": [
    "### Code ###\n",
    "#Find all the columns\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\"\n",
    "\n",
    "#Read CSV file from the url and parse it into a list of dictionaries\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    data = [row for row in csv.DictReader(response.read().decode(\"utf-8\").splitlines())]\n",
    "    \n",
    "print(\"Column Names: \", list(data[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of <NUM> tokens: 2487\n",
      "Number of <DATE> tokens: 40\n",
      "Number of <URL> tokens: 329\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import io\n",
    "import requests\n",
    "import re\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\"\n",
    "response = requests.get(url)\n",
    "content = response.content.decode(\"utf-8\")\n",
    "\n",
    "def count_tokens(rows):\n",
    "    num_count = 0\n",
    "    url_count = 0\n",
    "    date_count = 0\n",
    "    for row in rows:\n",
    "        content = row['content']\n",
    "        num_count += content.count(\"NUM\") #count number of \"<NUM>\" in column 'content'\n",
    "        date_count += content.count(\"DATE\") #count number of \"<DATE>\" in column 'content'\n",
    "        url_count += content.count(\"URL\") #count number of \"<URL>\" in column 'content'\n",
    "    return num_count, date_count, url_count\n",
    "\n",
    "rows = []\n",
    "for line in csv.DictReader(io.StringIO(content)):\n",
    "    line['content'] = clean_text_and_tokenize(line['content'])\n",
    "    rows.append(line)\n",
    "\n",
    "num_count, date_count, url_count = count_tokens(rows)\n",
    "\n",
    "print(f\"Number of <NUM> tokens: {num_count}\")\n",
    "print(f\"Number of <DATE> tokens: {date_count}\")\n",
    "print(f\"Number of <URL> tokens: {url_count}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #3\n",
    "\n",
    "Apply your data preprocessing pipeline to a larger proportion of the FakeNewsCorpus https://github.com/several27/FakeNewsCorpus/releases/tag/v1.0\n",
    "\n",
    "You may find it challenging to run your data processing pipeline on the entire FakeNewsCorpus. At a minimum, you should be able to process 10% of the data using your pipeline,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'news_cleaned_2018_02_13.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[472], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(stemmed_tokens)\n\u001b[0;32m     11\u001b[0m columns_to_process \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmeta_description\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdomain\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmeta_keyboards\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[1;32m---> 12\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mnews_cleaned_2018_02_13.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     13\u001b[0m     reader \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mDictReader(f)\n\u001b[0;32m     15\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mnews_cleaned_2018_02_13-results3.csv\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m fOut:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'news_cleaned_2018_02_13.csv'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def process_text(text):\n",
    "    if text is None:\n",
    "        return ''\n",
    "    tokens = clean_text_and_tokenize(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    stemmed_tokens = stem_tokens(tokens)\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "columns_to_process = {'content', 'type', 'meta_description', 'domain', 'title', 'meta_keyboards'}\n",
    "with open('news_cleaned_2018_02_13.csv', encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "\n",
    "    with open('news_cleaned_2018_02_13-results3.csv', 'w', encoding=\"utf-8\") as fOut:\n",
    "        fieldnames = reader.fieldnames + [\"processed_text\"]\n",
    "        writer = csv.DictWriter(fOut, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in reader:\n",
    "            processed_row = {column_name: (process_text(cell_value) if column_name in columns_to_process else cell_value) for column_name, cell_value in row.items()}\n",
    "            writer.writerow(processed_row)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #4\n",
    "\n",
    "Split the resulting dataset into a training, validation, and test splits. A common strategy is to uniformly at random split the data 80% / 10% / 10%. You will use the training data to train your baseline and advanced models, the validation data can be used for model selection and hyperparameter tuning, while the test data should only be used in Part 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask_ml.model_selection\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a Dask DataFrame with specified data types\n",
    "cleaned_data = dd.read_csv('news_cleaned_2018_02_13-results.csv', encoding=\"utf-8\", dtype={\n",
    "        'Unnamed: 0': 'object',\n",
    "        'id': 'object',\n",
    "        'domain': 'object',\n",
    "        'type': 'object',\n",
    "        'url': 'object',\n",
    "        'content': 'object',\n",
    "        'scraped_at': 'object',\n",
    "        'inserted_at': 'object',\n",
    "        'updated_at': 'object',\n",
    "        'title': 'object',\n",
    "        'authors': 'object',\n",
    "        'keywords': 'float64',\n",
    "        'meta_keywords': 'object',\n",
    "        'meta_description': 'object',\n",
    "        'tags': 'object',\n",
    "        'summary': 'float64',\n",
    "        'tokens': 'object',\n",
    "        'filtered_tokens': 'object',\n",
    "        'stemmed_tokens': 'object',\n",
    "    },)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to modify the 'type' column values\n",
    "def modify_type(x):\n",
    "    if x == 'reliabl':\n",
    "        return 'reliabl'\n",
    "    else:\n",
    "        return 'fake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the modify_type function to the 'type' column\n",
    "cleaned_data['type'] = cleaned_data['type'].map(modify_type, meta=('type', 'object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_to_empty(x):\n",
    "    if isinstance(x, float) and np.isnan(x):\n",
    "        return ''\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data['content'] = cleaned_data['content'].map(nan_to_empty, meta=('content', 'object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the 'type' column\n",
    "y = cleaned_data['type']\n",
    "X = cleaned_data['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = dask_ml.model_selection.train_test_split(X, y, test_size=0.2, random_state=0, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = dask_ml.model_selection.train_test_split(X_train, y_train, test_size=0.5, random_state=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorization = TfidfVectorizer()\n",
    "xv_train = vectorization.fit_transform(X_train)\n",
    "xv_test = vectorization.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Agnes/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression() \n",
    "model.fit(xv_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9947407563320003"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(xv_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model = model.predict(xv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.99      1.00      1.00    707312\n",
      "     reliabl       0.90      0.33      0.49      5338\n",
      "\n",
      "    accuracy                           0.99    712650\n",
      "   macro avg       0.95      0.67      0.74    712650\n",
      "weighted avg       0.99      0.99      0.99    712650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test,pred_model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
