{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now evaluate your models on the FakeNews and the LIAR dataset. Arrange all these results in a table to facilitate a comparison between them. You should be evaluating the model on how well it classifies articles correctly using F-score. You may want to include a confusion matrix to visualize the types of classification errors made by your models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "Evaluate the performance of your Simple and Advanced Models on your FakeNewsCorpus test set. It should be possible to achieve > 80% accuracy but you will not fail the project if your model cannot reach this performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "# pip install dask-ml\n",
    "import dask.dataframe as dd\n",
    "import dask_ml.model_selection\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a Dask DataFrame with specified data types\n",
    "# These dtypes can be found by printing the dask dataframe\n",
    "cleaned_data = dd.read_csv('news_cleaned_2018_02_13-results_200MB.csv', encoding=\"utf-8\", dtype={\n",
    "        'Unnamed: 0': 'object',\n",
    "        'id': 'object',\n",
    "        'domain': 'object',\n",
    "        'type': 'object',\n",
    "        'url': 'object',\n",
    "        'content': 'object',\n",
    "        'scraped_at': 'object',\n",
    "        'inserted_at': 'object',\n",
    "        'updated_at': 'object',\n",
    "        'title': 'object',\n",
    "        'authors': 'object',\n",
    "        'keywords': 'float64',\n",
    "        'meta_keywords': 'object',\n",
    "        'meta_description': 'object',\n",
    "        'tags': 'object',\n",
    "        'summary': 'float64',\n",
    "        'tokens': 'object',\n",
    "        'filtered_tokens': 'object',\n",
    "        'stemmed_tokens': 'object',\n",
    "    },)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to modify the 'type' column values\n",
    "# This function is used to simplify the classification problem\n",
    "# The 'reliable' and 'political' types are combined into a single 'reliable' type\n",
    "def modify_type(x):\n",
    "    if x == 'reliabl' or x == 'polit':\n",
    "        return '1'\n",
    "    else:\n",
    "        return '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the modify_type function to the 'type' column using the map function\n",
    "cleaned_data['type'] = cleaned_data['type'].map(modify_type, meta=('type', 'object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify if there are any missing values in the 'type' column\n",
    "# If so, they are replaced with an empty string\n",
    "def nan_to_empty(x):\n",
    "    if isinstance(x, float) and np.isnan(x):\n",
    "        return ''\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the nan_to_empty function to the 'type' column using the map function\n",
    "cleaned_data['content'] = cleaned_data['content'].map(nan_to_empty, meta=('content', 'object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y as the 'content' and 'type' columns\n",
    "y = cleaned_data['type']\n",
    "X = cleaned_data['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "# In an 80/10/10 split\n",
    "\n",
    "X_train, X_test, y_train, y_test = dask_ml.model_selection.train_test_split(X, y, test_size=0.2, random_state=0, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = dask_ml.model_selection.train_test_split(X_train, y_train, test_size=0.5, random_state=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TfidfVectorizer object for use in feature extraction\n",
    "vectorization = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the training data and transform the training data into a vector\n",
    "# As well as the validation data\n",
    "xv_train = vectorization.fit_transform(X_train)\n",
    "xv_test = vectorization.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=1500, class_weight = 'balanced', random_state = 0, C=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100, class_weight='balanced', max_iter=1500,\n",
       "                   random_state=0)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xv_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8921992743511025"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the accuracy of the model on the validation data\n",
    "model.score(xv_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class of the validation data\n",
    "pred_model = model.predict(xv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93     10668\n",
      "           1       0.77      0.82      0.80      3664\n",
      "\n",
      "    accuracy                           0.89     14332\n",
      "   macro avg       0.85      0.87      0.86     14332\n",
      "weighted avg       0.90      0.89      0.89     14332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print the classification report for the model\n",
    "print(classification_report(y_test,pred_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_mat = confusion_matrix(y_val, pred_model)\n",
    "\n",
    "# Plot the confusion matrix using Seaborn\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.set(font_scale=1.4) # for label size\n",
    "sns.heatmap(conf_mat, annot=True, annot_kws={\"size\": 16}, fmt=\"d\", cmap=\"Blues\", xticklabels=['Fake', 'Reliable'], yticklabels=['Fake', 'Reliable'])\n",
    "\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report \n",
    "from imblearn.over_sampling import RandomOverSampler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the features and labels for the model\n",
    "\n",
    "features = pd.read_csv('features_cleaned_multi.csv') \n",
    "labels = pd.read_csv('labels_multi.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizes the features and saves them as sparse matrices\n",
    "\n",
    "title_vectorizer = CountVectorizer(ngram_range=(2, 2), analyzer='word')\n",
    "sparse_matrix_for_title = title_vectorizer.fit_transform(features['title'])\n",
    "sparse.save_npz(\"sparse_matrix_for_title.npz\", sparse_matrix_for_title)\n",
    "sparse_matrix_for_title_load = sparse.load_npz(\"sparse_matrix_for_title.npz\")   \n",
    "\n",
    "content_vectorizer = CountVectorizer(ngram_range=(2, 2), analyzer='word')\n",
    "sparse_matrix_for_content = content_vectorizer.fit_transform(features['content'])\n",
    "sparse.save_npz(\"sparse_matrix_for_content.npz\", sparse_matrix_for_content)\n",
    "sparse_matrix_for_content_load = sparse.load_npz(\"sparse_matrix_for_content.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combines the sparse matrices into one sparse matrix\n",
    "matrix = hstack([sparse_matrix_for_content_load,sparse_matrix_for_title_load]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates our X and y variables\n",
    "X = matrix \n",
    "y = labels \n",
    "y = np.ravel(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the data into training, validation, and test sets using a 80/10/10 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.5, random_state=0)\n",
    "\n",
    "# Oversamples the training data to balance the classes\n",
    "oversampler = RandomOverSampler(random_state=0)\n",
    "X_train_oversampled, y_train_oversampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Creates the Naive Bayes model and prints the accuracy score\n",
    "model = ComplementNB(alpha=0.1) \n",
    "model.fit(X_train_oversampled, y_train_oversampled)\n",
    "\n",
    "print(model.score(X_train_oversampled, y_train_oversampled))\n",
    "\n",
    "predictions_NB = model.predict(X_test)\n",
    "\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, y_test)*100)\n",
    "\n",
    "y_test_pred = model.predict(X_test) \n",
    "\n",
    "report = classification_report(y_test, y_test_pred)\n",
    "print(\"classification report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "# Create the heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='g', cmap='coolwarm', linewidths=0.5, square=True, cbar=False, xticklabels=True, yticklabels=True)\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "In order to allow you to play around cross-domain performance, try the same exercise on the LIAR dataset, where you know the labels, and can thus immediately calculate the performance. You are expected to directly evaluate the model you trained on the FakeNewsCorpus. In other words, you do not need to retrain the model on the LIAR dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "# pip install dask-ml\n",
    "import dask.dataframe as dd\n",
    "import dask_ml.model_selection\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a Dask DataFrame with specified data types\n",
    "# These dtypes can be found by printing the dask dataframe\n",
    "LIAR_features = dd.read_csv('features_cleaned_LIAR.csv', encoding=\"utf-8\", dtype={'content' : 'object'},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a Dask DataFrame with specified data types\n",
    "# These dtypes can be found by printing the dask dataframe\n",
    "LIAR_labels = dd.read_csv('labels_LIAR.csv', encoding=\"utf-8\", dtype={'label' : 'object'},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y as the 'content' and 'type' columns\n",
    "y_test = LIAR_labels['label']\n",
    "X_test = LIAR_features['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the training data into a vector\n",
    "xv_test = vectorization.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4135753749013418"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the accuracy of the model on the validation data\n",
    "model.score(xv_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class of the validation data\n",
    "pred_model = model.predict(xv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.37      0.51      1059\n",
      "           1       0.17      0.65      0.27       208\n",
      "\n",
      "    accuracy                           0.41      1267\n",
      "   macro avg       0.51      0.51      0.39      1267\n",
      "weighted avg       0.73      0.41      0.47      1267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print the classification report for the model\n",
    "print(classification_report(y_test,pred_model))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced model LIAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "# pip install dask-ml\n",
    "import dask.dataframe as dd\n",
    "import dask_ml.model_selection\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a Dask DataFrame with specified data types\n",
    "# These dtypes can be found by printing the dask dataframe\n",
    "cleaned_data = dd.read_csv('news_cleaned_2018_02_13-results_200MB.csv', encoding=\"utf-8\", dtype={\n",
    "        'Unnamed: 0': 'object',\n",
    "        'id': 'object',\n",
    "        'domain': 'object',\n",
    "        'type': 'object',\n",
    "        'url': 'object',\n",
    "        'content': 'object',\n",
    "        'scraped_at': 'object',\n",
    "        'inserted_at': 'object',\n",
    "        'updated_at': 'object',\n",
    "        'title': 'object',\n",
    "        'authors': 'object',\n",
    "        'keywords': 'float64',\n",
    "        'meta_keywords': 'object',\n",
    "        'meta_description': 'object',\n",
    "        'tags': 'object',\n",
    "        'summary': 'float64',\n",
    "        'tokens': 'object',\n",
    "        'filtered_tokens': 'object',\n",
    "        'stemmed_tokens': 'object',\n",
    "    },)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to modify the 'type' column values\n",
    "# This function is used to simplify the classification problem\n",
    "# The 'reliable' and 'political' types are combined into a single 'reliable' type\n",
    "def modify_type(x):\n",
    "    if x == 'reliabl' or x == 'polit':\n",
    "        return '1'\n",
    "    else:\n",
    "        return '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the modify_type function to the 'type' column using the map function\n",
    "cleaned_data['type'] = cleaned_data['type'].map(modify_type, meta=('type', 'object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify if there are any missing values in the 'type' column\n",
    "# If so, they are replaced with an empty string\n",
    "def nan_to_empty(x):\n",
    "    if isinstance(x, float) and np.isnan(x):\n",
    "        return ''\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the nan_to_empty function to the 'type' column using the map function\n",
    "cleaned_data['content'] = cleaned_data['content'].map(nan_to_empty, meta=('content', 'object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y as the 'content' and 'type' columns\n",
    "y = cleaned_data['type']\n",
    "X = cleaned_data['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "# In an 80/10/10 split\n",
    "\n",
    "X_train, X_test, y_train, y_test = dask_ml.model_selection.train_test_split(X, y, test_size=0.2, random_state=0, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = dask_ml.model_selection.train_test_split(X_train, y_train, test_size=0.5, random_state=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "# pip install dask-ml\n",
    "import dask.dataframe as dd\n",
    "import dask_ml.model_selection\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a Dask DataFrame with specified data types\n",
    "# These dtypes can be found by printing the dask dataframe\n",
    "LIAR_features = dd.read_csv('features_cleaned_LIAR_multi.csv', encoding=\"utf-8\", dtype={'content' : 'object'},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a Dask DataFrame with specified data types\n",
    "# These dtypes can be found by printing the dask dataframe\n",
    "LIAR_labels = dd.read_csv('labels_LIAR_multi.csv', encoding=\"utf-8\", dtype={'label' : 'object'},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y as the 'content' and 'type' columns\n",
    "y_test = LIAR_labels['label']\n",
    "X_test = LIAR_features['content']\n",
    "y_test = np.ravel(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TfidfVectorizer object for use in feature extraction\n",
    "vectorization = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the training data and transform the training data into a vector\n",
    "xv_train = vectorization.fit_transform(X_train)\n",
    "xv_test = vectorization.transform(X_test)\n",
    "# y_train = vectorization.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ComplementNB(alpha=0.1) \n",
    "model.fit(xv_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(xv_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class of the validation data\n",
    "pred_model = model.predict(xv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print the classification report for the model\n",
    "print(classification_report(y_test,pred_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "Compare the results of this experiment to the results you obtained in question 3. Report your LIAR results as part of your report. Remember to test the performance of your Simple Model as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
