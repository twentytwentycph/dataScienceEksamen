{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "Create one or more reasonable baselines for Fake News predictor. Aim to train a binary classification model that can predict whether an article is reliable or fake"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #0\n",
    "- Discuss how you grouped the labels into two groups. \n",
    "- Are there any limitations that could arise from the decision we've made when grouping the labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We've grouped the labels into two groups, 'content' and 'type', to optimize our fake news predictor in defining whether the article is reliable or unreliable. \n",
    "2. The decision to group labels into two groups can have limitations, primarily if the labels have overlapping information. This doesn't seem to be the case in our situation, \n",
    "3. So we tried to label it as reliable and fake, but we came across the problem that we had stemmed 'reliable' so it could not label all the articles, so we made a if statement instead"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #1\n",
    "- Start by considering only features extracted from 'content' column. \n",
    "- Choose one or more simple baseline models, train them, and report accuracies. \n",
    "- Report necessary details about baseline models (choice of relevant parameters and how you chose them). \n",
    "- Describe why you chose these baseline models - why are they reasonable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "# pip install dask-ml\n",
    "import dask.dataframe as dd\n",
    "import dask_ml.model_selection\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a Dask DataFrame with specified data types\n",
    "# These dtypes can be found by printing the dask dataframe\n",
    "cleaned_data = dd.read_csv('data_3GB.csv', encoding=\"utf-8\", dtype={\n",
    "        'Unnamed: 0': 'object',\n",
    "        'id': 'object',\n",
    "        'domain': 'object',\n",
    "        'type': 'object',\n",
    "        'url': 'object',\n",
    "        'content': 'object',\n",
    "        'scraped_at': 'object',\n",
    "        'inserted_at': 'object',\n",
    "        'updated_at': 'object',\n",
    "        'title': 'object',\n",
    "        'authors': 'object',\n",
    "        'keywords': 'float64',\n",
    "        'meta_keywords': 'object',\n",
    "        'meta_description': 'object',\n",
    "        'tags': 'object',\n",
    "        'summary': 'float64',\n",
    "        'tokens': 'object',\n",
    "        'filtered_tokens': 'object',\n",
    "        'stemmed_tokens': 'object',\n",
    "    },)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to modify the 'type' column values\n",
    "# This function is used to simplify the classification problem\n",
    "# The 'reliable' and 'political' types are combined into a single 'reliable' type\n",
    "def modify_type(x):\n",
    "    if x == 'reliabl' or x == 'polit':\n",
    "        return 'reliable'\n",
    "    else:\n",
    "        return 'fake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the modify_type function to the 'type' column using the map function\n",
    "cleaned_data['type'] = cleaned_data['type'].map(modify_type, meta=('type', 'object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify if there are any missing values in the 'type' column\n",
    "# If so, they are replaced with an empty string\n",
    "def nan_to_empty(x):\n",
    "    if isinstance(x, float) and np.isnan(x):\n",
    "        return ''\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the nan_to_empty function to the 'type' column using the map function\n",
    "cleaned_data['content'] = cleaned_data['content'].map(nan_to_empty, meta=('content', 'object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y as the 'content' and 'type' columns\n",
    "y = cleaned_data['type']\n",
    "X = cleaned_data['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "# In an 80/10/10 split\n",
    "\n",
    "X_train, X_test, y_train, y_test = dask_ml.model_selection.train_test_split(X, y, test_size=0.2, random_state=0, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = dask_ml.model_selection.train_test_split(X_train, y_train, test_size=0.125, random_state=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TfidfVectorizer object for use in feature extraction\n",
    "vectorization = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the training data and transform the training data into a vector\n",
    "# As well as the validation data\n",
    "xv_train = vectorization.fit_transform(X_train)\n",
    "xv_val = vectorization.transform(X_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define a range of C values to test\n",
    "C_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "# Initialize the best_C and best_score variables\n",
    "best_C = None\n",
    "best_score = 0\n",
    "\n",
    "# Iterate through the C_values and fit the model with each value\n",
    "for C in C_values:\n",
    "    # Create a Logistic Regression model with the current C value\n",
    "    model = LogisticRegression(max_iter=1500, class_weight = 'balanced', random_state = 0, C=C)\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    model.fit(xv_train, y_train)\n",
    "    \n",
    "    # Predict the validation data\n",
    "    y_val_pred = model.predict(xv_val)\n",
    "    \n",
    "    # Calculate the accuracy score for the current C value\n",
    "    score = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Print the current C value and its accuracy score\n",
    "    print(f\"C: {C}, Accuracy: {score}\")\n",
    "    \n",
    "    # Update the best_C and best_score variables if the current score is higher than the previous best\n",
    "    if score > best_score:\n",
    "        best_C = C\n",
    "        best_score = score\n",
    "\n",
    "# Print the best C value and its accuracy score\n",
    "print(f\"Best C: {best_C}, Best Accuracy: {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the accuracy of the model on the validation data\n",
    "model.score(xv_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class of the validation data\n",
    "pred_model = model.predict(xv_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print the classification report for the model\n",
    "print(classification_report(y_val,pred_model))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #2\n",
    "- Consider whether it would make sense to include meta-data features as well. If so, which ones, and why? \n",
    "- If relevant, report performance when including these additional features and compare it to the first baselines. \n",
    "- Discuss whether these results match expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remainder of the project, we will limit ourselves to main-text data only (i.e. no meta-data). This makes it easier to do the cross-domain experiment in Part 4 (which does not have the same set of meta-data fields)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
