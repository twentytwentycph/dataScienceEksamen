{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "Create one or more reasonable baselines for Fake News predictor. Aim to train a binary classification model that can predict whether an article is reliable or fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Code###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #0\n",
    "- Discuss how you grouped the labels into two groups. \n",
    "- Are there any limitations that could arise from the decision we've made when grouping the labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We've grouped the labels into two groups, 'content' and 'type', to optimize our fake news predictor in defining whether the article is reliable or unreliable. \n",
    "2. The decision to group labels into two groups can have limitations, primarily if the labels have overlapping information. This doesn't seem to be the case in our situation, \n",
    "3. So we tried to label it as reliable and fake, but we came across the problem that we had stemmed 'reliable' so it could not label all the articles, so we made a if statement instead"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #1\n",
    "- Start by considering only features extracted from 'content' column. \n",
    "- Choose one or more simple baseline models, train them, and report accuracies. \n",
    "- Report necessary details about baseline models (choice of relevant parameters and how you chose them). \n",
    "- Describe why you chose these baseline models - why are they reasonable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #2\n",
    "- Consider whether it would make sense to include meta-data features as well. If so, which ones, and why? \n",
    "- If relevant, report performance when including these additional features and compare it to the first baselines. \n",
    "- Discuss whether these results match expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remainder of the project, we will limit ourselves to main-text data only (i.e. no meta-data). This makes it easier to do the cross-domain experiment in Part 4 (which does not have the same set of meta-data fields)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
